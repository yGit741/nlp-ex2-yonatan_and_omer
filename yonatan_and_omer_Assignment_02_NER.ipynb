{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yGit741/nlp-ex2-yonatan_and_omer/blob/main/yonatan_and_omer_Assignment_02_NER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-WJBimYDLJS"
      },
      "source": [
        "# Natural Language Processing\n",
        "![](https://i.imgur.com/qkg2E2D.png)\n",
        "\n",
        "## Assignment 002 - NER Tagger\n",
        "\n",
        "> Notebook by:\n",
        "> - NLP Course Stuff\n",
        "## Revision History\n",
        "\n",
        "| Version | Date       | User        | Content / Changes                                                   |\n",
        "|---------|------------|-------------|---------------------------------------------------------------------|\n",
        "| 0.1.000 | 29/05/2025 | course staff| First version                                                       |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-fCqGh9ybgm"
      },
      "source": [
        "## Overview\n",
        "In this assignment, you will build a complete training and testing pipeline for a neural sequential tagger for named entities using LSTM.\n",
        "\n",
        "## Dataset\n",
        "You will work with the ReCoNLL 2003 dataset, a corrected version of the [CoNLL 2003 dataset](https://www.clips.uantwerpen.be/conll2003/ner/):\n",
        "\n",
        "**Click on those links so you have access to the data!**\n",
        "- [Train data](https://drive.google.com/file/d/1CqEGoLPVKau3gvVrdG6ORyfOEr1FSZGf/view?usp=sharing)\n",
        "\n",
        "- [Dev data](https://drive.google.com/file/d/1rdUida-j3OXcwftITBlgOh8nURhAYUDw/view?usp=sharing)\n",
        "\n",
        "- [Test data](https://drive.google.com/file/d/137Ht40OfflcsE6BIYshHbT5b2iIJVaDx/view?usp=sharing)\n",
        "\n",
        "As you will see, the annotated texts are labeled according to the `IOB` annotation scheme (more on this below), for 3 entity types: Person, Organization, Location.\n",
        "\n",
        "## Your Implementation\n",
        "\n",
        "Please create a local copy of this template Colab's Notebook:\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1KGkObwUn5QQm_v0nB0nAUlB4YrwThuzl#scrollTo=Z-fCqGh9ybgm)\n",
        "\n",
        "The assignment's instructions are there; follow the notebook.\n",
        "\n",
        "## Submission\n",
        "- **Notebook Link**: Add the URL to your assignment's notebook in the `notebook_link.txt` file, following the format provided in the example.\n",
        "- **Access**: Ensure the link has edit permissions enabled to allow modifications if needed.\n",
        "- **Deadline**: <font color='green'>12/06/2025</font>.\n",
        "- **Platform**: Continue using GitHub for submissions. Push your project to the team repository and monitor the test results under the actions section.\n",
        "\n",
        "Good Luck ü§ó\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOy8IghquR6x"
      },
      "source": [
        "<!-- ## NER schemes:  \n",
        "\n",
        "> `IO`: is the simplest scheme that can be applied to this task. In this scheme, each token from the dataset is assigned one of two tags: an inside tag (`I`) and an outside tag (`O`). The `I` tag is for named entities, whereas the `O` tag is for normal words. This scheme has a limitation, as it cannot correctly encode consecutive entities of the same type.\n",
        "\n",
        "> `IOB`: This scheme is also referred to in the literature as BIO and has been adopted by the Conference on Computational Natural Language Learning (CoNLL) [1]. It assigns a tag to each word in the text, determining whether it is the beginning (`B`) of a known named entity, inside (`I`) it, or outside (`O`) of any known named entities.\n",
        "\n",
        "> `IOE`: This scheme works nearly identically to `IOB`, but it indicates the end of the entity (`E` tag) instead of its beginning.\n",
        "\n",
        "> `IOBES`: An alternative to the IOB scheme is `IOBES`, which increases the amount of information related to the boundaries of named entities. In addition to tagging words at the beginning (`B`), inside (`I`), end (`E`), and outside (`O`) of a named entity. It also labels single-token entities with the tag `S`.\n",
        "\n",
        "> `BI`: This scheme tags entities in a similar method to `IOB`. Additionally, it labels the beginning of non-entity words with the tag B-O and the rest as I-O.\n",
        "\n",
        "> `IE`: This scheme works exactly like `IOE` with the distinction that it labels the end of non-entity words with the tag `E-O` and the rest as `I-O`.\n",
        "\n",
        "> `BIES`: This scheme encodes the entities similar to `IOBES`. In addition, it also encodes the non-entity words using the same method. It uses `B-O` to tag the beginning of non-entity words, `I-O` to tag the inside of non-entity words, and `S-O` for single non-entity tokens that exist between two entities. -->\n",
        "\n",
        "\n",
        "## NER Schemes\n",
        "\n",
        "### IO\n",
        "- **Description**: The simplest scheme for named entity recognition (NER).\n",
        "- **Tags**:\n",
        "  - `I`: Inside a named entity.\n",
        "  - `O`: Outside any named entity.\n",
        "- **Limitation**: Cannot correctly encode consecutive entities of the same type.\n",
        "\n",
        "### IOB (BIO)\n",
        "- **Description**: Adopted by the Conference on Computational Natural Language Learning (CoNLL).\n",
        "- **Tags**:\n",
        "  - `B`: Beginning of a named entity.\n",
        "  - `I`: Inside a named entity.\n",
        "  - `O`: Outside any named entity.\n",
        "- **Advantage**: Can encode the boundaries of consecutive entities.\n",
        "\n",
        "### IOE\n",
        "- **Description**: Similar to IOB, but indicates the end of an entity.\n",
        "- **Tags**:\n",
        "  - `I`: Inside a named entity.\n",
        "  - `O`: Outside any named entity.\n",
        "  - `E`: End of a named entity.\n",
        "- **Advantage**: Focuses on the end boundary of entities.\n",
        "\n",
        "### IOBES\n",
        "- **Description**: An extension of IOB with additional boundary information.\n",
        "- **Tags**:\n",
        "  - `B`: Beginning of a named entity.\n",
        "  - `I`: Inside a named entity.\n",
        "  - `O`: Outside any named entity.\n",
        "  - `E`: End of a named entity.\n",
        "  - `S`: Single-token named entity.\n",
        "- **Advantage**: Provides more detailed boundary information for named entities.\n",
        "\n",
        "### BI\n",
        "- **Description**: Tags entities similarly to IOB and labels the beginning of non-entity words.\n",
        "- **Tags**:\n",
        "  - `B`: Beginning of a named entity.\n",
        "  - `I`: Inside a named entity.\n",
        "  - `B-O`: Beginning of a non-entity word.\n",
        "  - `I-O`: Inside a non-entity word.\n",
        "- **Advantage**: Distinguishes the beginning of non-entity sequences.\n",
        "\n",
        "### IE\n",
        "- **Description**: Similar to IOE but for non-entity words.\n",
        "- **Tags**:\n",
        "  - `I`: Inside a named entity.\n",
        "  - `O`: Outside any named entity.\n",
        "  - `E`: End of a named entity.\n",
        "  - `E-O`: End of a non-entity word.\n",
        "  - `I-O`: Inside a non-entity word.\n",
        "- **Advantage**: Highlights the end of non-entity sequences.\n",
        "\n",
        "### BIES\n",
        "- **Description**: Encodes both entities and non-entity words using the IOBES method.\n",
        "- **Tags**:\n",
        "  - `B`: Beginning of a named entity.\n",
        "  - `I`: Inside a named entity.\n",
        "  - `O`: Outside any named entity.\n",
        "  - `E`: End of a named entity.\n",
        "  - `S`: Single-token named entity.\n",
        "  - `B-O`: Beginning of a non-entity word.\n",
        "  - `I-O`: Inside a non-entity word.\n",
        "  - `S-O`: Single non-entity token.\n",
        "- **Advantage**: Comprehensive encoding for both entities and non-entities.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRwONXCzi28v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa8c8f04-c111-44bf-939a-3394d0ffc3ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‚Äòdata‚Äô: File exists\n"
          ]
        }
      ],
      "source": [
        "!mkdir data\n",
        "# Fetch data\n",
        "train_link = 'https://drive.google.com/file/d/1CqEGoLPVKau3gvVrdG6ORyfOEr1FSZGf/view?usp=sharing'\n",
        "dev_link   = 'https://drive.google.com/file/d/1rdUida-j3OXcwftITBlgOh8nURhAYUDw/view?usp=sharing'\n",
        "test_link  = 'https://drive.google.com/file/d/137Ht40OfflcsE6BIYshHbT5b2iIJVaDx/view?usp=sharing'\n",
        "\n",
        "!wget -q --no-check-certificate 'https://docs.google.com/uc?export=download&id=1CqEGoLPVKau3gvVrdG6ORyfOEr1FSZGf' -O data/train.txt\n",
        "!wget -q --no-check-certificate 'https://docs.google.com/uc?export=download&id=1rdUida-j3OXcwftITBlgOh8nURhAYUDw' -O data/dev.txt\n",
        "!wget -q --no-check-certificate 'https://docs.google.com/uc?export=download&id=137Ht40OfflcsE6BIYshHbT5b2iIJVaDx' -O data/test.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QNUSyEwvWqn"
      },
      "outputs": [],
      "source": [
        "# Any additional needed libraries\n",
        "# !pip install --q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3enPCGBF8FlX"
      },
      "outputs": [],
      "source": [
        "# Standard Library Imports\n",
        "import os\n",
        "import copy\n",
        "import random\n",
        "import warnings\n",
        "from collections import defaultdict\n",
        "from typing import Optional\n",
        "\n",
        "# ML\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import pandas as pd\n",
        "\n",
        "# Visual\n",
        "import matplotlib\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "from tabulate import tabulate\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display\n",
        "\n",
        "# DL\n",
        "import torch as th\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# Metrics\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score , roc_auc_score, classification_report, confusion_matrix, precision_recall_fscore_support\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZUM4WJ9PwF0x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4322a44-d54a-4b83-c977-998e95c938f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "SEED = 42\n",
        "# Set the random seed for Python\n",
        "random.seed(SEED)\n",
        "\n",
        "# Set the random seed for numpy\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# Set the random seed for pytorch\n",
        "th.manual_seed(SEED)\n",
        "\n",
        "# If using CUDA (for GPU operations)\n",
        "th.cuda.manual_seed(SEED)\n",
        "\n",
        "# Set up the device\n",
        "# TO DO ----------------------------------------------------------------------\n",
        "DEVICE = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", DEVICE)\n",
        "# TO DO ----------------------------------------------------------------------\n",
        "assert DEVICE.type == \"cuda\", \"GPU not being used!\"\n",
        "\n",
        "DataType = list[tuple[list[str], list[str]]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-1shPaJ0z1B"
      },
      "source": [
        "# Part 1 - Dataset Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ul2Y3vuPoV8"
      },
      "source": [
        "## Step 1: Read Data\n",
        "Write a function for reading the data from a single file (of the ones that are provided above).   \n",
        "- The function recieves a filepath\n",
        "- The funtion encodes every sentence individually using a pair of lists, one list contains the words and one list contains the tags.\n",
        "- Each list pair will be added to a general list (data), which will be returned back from the function.\n",
        "\n",
        "Example output:\n",
        "```\n",
        "[\n",
        "  (['At','Trent','Bridge',':'],['O','B-LOC','I-LOC ','O']),\n",
        "  ([...],[...]),\n",
        "  ...\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prgzgtt8Jw4Y"
      },
      "outputs": [],
      "source": [
        "def read_data(filepath:str) -> DataType:\n",
        "  \"\"\"\n",
        "  Read data from a single file.\n",
        "  The function recieves a filepath\n",
        "  The funtion encodes every sentence using a pair of lists, one list contains the words and one list contains the tags.\n",
        "  :param filepath: path to the file\n",
        "  :return: data as a list of tuples\n",
        "  \"\"\"\n",
        "  data = []\n",
        "  # TO DO ----------------------------------------------------------------------\n",
        "  sentence_words = []\n",
        "  sentence_tags = []\n",
        "\n",
        "  with open(filepath, 'r') as f:\n",
        "    for line in f: # read line by line from the file\n",
        "      line = line.strip()\n",
        "\n",
        "      if line:  # check the line isn't empty\n",
        "        parts = line.split() # split the line to word and a tag\n",
        "        if len(parts) == 2: # verify format\n",
        "          word, tag = parts\n",
        "          sentence_words.append(word)\n",
        "          sentence_tags.append(tag)\n",
        "        else:\n",
        "          print(f\"Problem with format at line: {line} - line was skipped\")\n",
        "\n",
        "      else: # if empty line it should be the end of a sentence\n",
        "        if len(sentence_words) == len(sentence_tags): # verify that we have the same amount of tags and words\n",
        "              data.append((sentence_words, sentence_tags)) # add new tagged sentence in the desired format\n",
        "        else:\n",
        "            print(f\"\"\"Error: Mismatched word and tag counts in sentence.\n",
        "              Skipping sentence: {sentence_words}\"\"\")\n",
        "        sentence_words = []\n",
        "        sentence_tags = []\n",
        "\n",
        "   # handle the last sentence if it doesn't end with an empty line\n",
        "  if sentence_words:\n",
        "      if len(sentence_words) == len(sentence_tags):\n",
        "          data.append((sentence_words, sentence_tags))\n",
        "      else:\n",
        "          print(f\"\"\"Error: Mismatched word and tag counts in last sentence.\n",
        "            Skipping sentence: {sentence_words}\"\"\")\n",
        "\n",
        "  # TO DO ----------------------------------------------------------------------\n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yURR0GmX2i8M"
      },
      "outputs": [],
      "source": [
        "train = read_data(\"data/train.txt\")\n",
        "dev = read_data(\"data/dev.txt\")\n",
        "test = read_data(\"data/test.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "################################## Our Tests ###################################\n",
        "\n",
        "# Verify the structure of the output from read_data\n",
        "train_data = read_data('data/train.txt')\n",
        "\n",
        "# Check if the output is a list of tuples\n",
        "is_output_list_of_tuples = isinstance(train_data, list) and all(isinstance(item, tuple) for item in train_data)\n",
        "\n",
        "# Check if each tuple contains two lists\n",
        "each_tuple_has_two_lists = all(len(item) == 2 and isinstance(item[0], list) and isinstance(item[1], list) for item in train_data)\n",
        "\n",
        "# Check if all items in the first list of each tuple are strings\n",
        "all_words_are_strings = all(all(isinstance(word, str) for word in sentence[0]) for sentence in train_data)\n",
        "\n",
        "# Check if all items in the second list of each tuple are strings\n",
        "all_tags_are_strings = all(all(isinstance(tag, str) for tag in sentence[1]) for sentence in train_data)\n",
        "\n",
        "# Check if the lengths of the word list and tag list are the same for each sentence\n",
        "word_tag_list_lengths_match = all(len(sentence[0]) == len(sentence[1]) for sentence in train_data)\n",
        "\n",
        "# check\n",
        "set([tag for pair in train for tag in pair[1]])\n",
        "\n",
        "# Print verification results with informative names and test manners\n",
        "print(f\"Test: Output is a list of tuples: {is_output_list_of_tuples}\")\n",
        "print(f\"Test: Each tuple contains two lists: {each_tuple_has_two_lists}\")\n",
        "print(f\"Test: All words in sentence lists are strings: {all_words_are_strings}\")\n",
        "print(f\"Test: All tags in tag lists are strings: {all_tags_are_strings}\")\n",
        "print(f\"Test: Word and tag lists have matching lengths for each sentence: {word_tag_list_lengths_match}\")"
      ],
      "metadata": {
        "id": "0A_Zt0ofGZ4f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "673eda62-dc67-47e4-a543-de228ebbe990"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test: Output is a list of tuples: True\n",
            "Test: Each tuple contains two lists: True\n",
            "Test: All words in sentence lists are strings: True\n",
            "Test: All tags in tag lists are strings: True\n",
            "Test: Word and tag lists have matching lengths for each sentence: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!-- ## Step 2: Create vocab\n",
        "\n",
        "The following `Vocab` class can be served as a dictionary that maps words and tags into Ids.   \n",
        "The `UNK_TOKEN` should be used for words that are not part of the training data.\n",
        "\n",
        "Note: you may change the Vocab class. -->\n",
        "\n",
        "## Step 2: Convert IOB Tags to IOBES Scheme\n",
        "\n",
        "### Your Task\n",
        "1. Implement the `iob_to_iobes` function to convert IOB tags to IOBES.\n",
        "2. Apply this conversion to the datasets.\n",
        "\n",
        "The IOBES scheme adds:\n",
        "- `E-X`: End of entity type X (the last token of a multi-token entity)\n",
        "- `S-X`: Single token entity of type X (both beginning and end)\n",
        "\n",
        "Example conversion:\n",
        "- `B-PER I-PER O` ‚Üí `B-PER E-PER O`\n",
        "- `B-LOC O` ‚Üí `S-LOC O`\n",
        "\n"
      ],
      "metadata": {
        "id": "RGnHLdxhEtIl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def iob_to_iobes(tags: list[str]) -> list[str]:\n",
        "  \"\"\"\n",
        "  Convert IOB tags to IOBES format.\n",
        "\n",
        "  The IOB format tags tokens as:\n",
        "  - B-X: Beginning of entity type X\n",
        "  - I-X: Inside entity type X\n",
        "  - O: Outside any entity\n",
        "\n",
        "  The IOBES format adds:\n",
        "  - E-X: End of entity type X\n",
        "  - S-X: Single token entity of type X\n",
        "\n",
        "  Args:\n",
        "      tags (list): List of IOB tags\n",
        "\n",
        "  Returns:\n",
        "      list: List of tags in IOBES format\n",
        "  \"\"\"\n",
        "  # TO DO ----------------------------------------------------------------------\n",
        "  iobes_list = []\n",
        "\n",
        "  # add 'O' to handle the last tag's\n",
        "  tags_with_sentinel = tags + ['O']\n",
        "\n",
        "  for current_tag, next_tag in zip(tags_with_sentinel[:-1], tags_with_sentinel[1:]):\n",
        "    # for O do nothing\n",
        "    if current_tag == 'O':\n",
        "      iobes_list.append('O')\n",
        "\n",
        "    # for Begining of entity make it S if it is single\n",
        "    elif current_tag.startswith('B-'):\n",
        "      entity_type = current_tag[2:]\n",
        "      # check if the next tag is an 'I-'\n",
        "      if next_tag.startswith('I-'):\n",
        "          next_entity_type = next_tag[2:]\n",
        "          # check if it's an 'I-' of the *same* entity type\n",
        "          if next_entity_type == entity_type:\n",
        "            iobes_list.append(current_tag) # it's the beginning of a multi-token entity\n",
        "          else:\n",
        "              # warning for B-X followed by I-Y (X != Y)\n",
        "              print(f\"\"\"Warning: Inconsistent tag sequence found: {current_tag}\n",
        "                    followed by {next_tag}. Converting {current_tag} to S-{entity_type}.\"\"\")\n",
        "              iobes_list.append(f'S-{entity_type}') # treat as single-token entity\n",
        "      else:\n",
        "        iobes_list.append(f'S-{entity_type}') # it's a single-token entity (followed by O, B- different type, or end of sequence)\n",
        "\n",
        "    # for Intermidiate of entity make it E if the next token is of differnt type\n",
        "    elif current_tag.startswith('I-'):\n",
        "      entity_type = current_tag[2:]\n",
        "      # check if the next tag continues the same entity\n",
        "      if next_tag.startswith('I-') and next_tag[2:] == entity_type:\n",
        "          iobes_list.append(current_tag) # It's inside a multi-token entity\n",
        "      else:\n",
        "          iobes_list.append(f'E-{entity_type}') # It's the end of a multi-token entity (followed by O, B- different type, or end of sequence)\n",
        "\n",
        "    # handle edge cases - where I tag appears without a preceding B or I of the same type\n",
        "    else:\n",
        "        # if we encounter an unexpected tag format (like an isolated I-), treat it as O and warn\n",
        "        print(f\"Warning: Encountered an unexpected or isolated tag format: {current_tag}. Treating as 'O'.\")\n",
        "        iobes_list.append('O')\n",
        "\n",
        "  # TO DO ----------------------------------------------------------------------\n",
        "  return iobes_list\n",
        "\n",
        "\n",
        "\n",
        "def convert_dataset_to_iobes(dataset: DataType) -> DataType:\n",
        "  \"\"\"\n",
        "  Convert a dataset from IOB to IOBES format.\n",
        "\n",
        "  Args:\n",
        "      dataset (DataType): Dataset in IOB format\n",
        "\n",
        "  Returns:\n",
        "      DataType: Dataset in IOBES format\n",
        "  \"\"\"\n",
        "  # TO DO ----------------------------------------------------------------------\n",
        "  iobes_dataset = []\n",
        "  for pair in dataset:\n",
        "    words, tags = pair\n",
        "    iobes_tags = iob_to_iobes(tags)\n",
        "    iobes_dataset.append((words, iobes_tags))\n",
        "  # TO DO ----------------------------------------------------------------------\n",
        "  return iobes_dataset"
      ],
      "metadata": {
        "id": "BPFKxdQxFRYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################################## Our Tests ###################################\n",
        "\n",
        "# Test 1: Basic multi-token entity\n",
        "test1_input = ['B-X', 'I-X', 'I-X', 'O']\n",
        "test1_expected = ['B-X', 'I-X', 'E-X', 'O']\n",
        "test1_success = iob_to_iobes(test1_input) == test1_expected\n",
        "\n",
        "# Test 2: Single-token entity\n",
        "test2_input = ['B-X', 'O']\n",
        "test2_expected = ['S-X', 'O']\n",
        "test2_success = iob_to_iobes(test2_input) == test2_expected\n",
        "\n",
        "# Test 3: I at end of sequence\n",
        "test3_input = ['B-X', 'I-X']\n",
        "test3_expected = ['B-X', 'E-X']\n",
        "test3_success = iob_to_iobes(test3_input) == test3_expected\n",
        "\n",
        "# Test 4: Inconsistent tags (B-X followed by I-Y)\n",
        "test4_input = ['B-X', 'I-Y']\n",
        "test4_expected = ['S-X', 'I-Y']  # Should show warning\n",
        "test4_success = iob_to_iobes(test4_input) == test4_expected\n",
        "\n",
        "# Test 5: Isolated I tag\n",
        "test5_input = ['I-X']\n",
        "test5_expected = ['O']  # Should show warning\n",
        "test5_success = iob_to_iobes(test5_input) == test5_expected\n",
        "\n",
        "# Test 6: Mixed entity types\n",
        "test6_input = ['B-X', 'I-X', 'B-Y', 'O']\n",
        "test6_expected = ['B-X', 'E-X', 'S-Y', 'O']\n",
        "test6_success = iob_to_iobes(test6_input) == test6_expected\n",
        "\n",
        "# Test 7: All O tags\n",
        "test7_input = ['O', 'O']\n",
        "test7_expected = ['O', 'O']\n",
        "test7_success = iob_to_iobes(test7_input) == test7_expected\n",
        "\n",
        "# Dataset conversion tests\n",
        "sample_dataset = [\n",
        "    ([\"Hello\", \"world\"], [\"O\", \"O\"]),\n",
        "    ([\"New\", \"York\", \"City\"], [\"B-LOC\", \"I-LOC\", \"I-LOC\"]),\n",
        "    ([\"Python\"], [\"B-LANG\"])\n",
        "]\n",
        "\n",
        "converted_dataset = convert_dataset_to_iobes(sample_dataset)\n",
        "\n",
        "# Structural checks\n",
        "is_dataset_list = isinstance(converted_dataset, list)\n",
        "all_tuples = all(isinstance(pair, tuple) for pair in converted_dataset)\n",
        "all_lengths_match = all(len(words) == len(tags) for (words, tags) in converted_dataset)\n",
        "valid_tags = all(tag in {'O', 'B-LOC', 'I-LOC', 'E-LOC', 'S-LOC', 'B-LANG', 'S-LANG'}  # Add all possible types here, or only check prefixes if not possible\n",
        "                for (_, tags_list) in converted_dataset\n",
        "                for tag in tags_list)\n",
        "\n",
        "# NOTE: For a general test, you can check that all tags are either 'O' or start with 'B-', 'I-', 'E-', 'S-':\n",
        "valid_tags_general = all(tag == 'O' or any(tag.startswith(prefix) for prefix in ['B-', 'I-', 'E-', 'S-'])\n",
        "                        for (_, tags_list) in converted_dataset\n",
        "                        for tag in tags_list)\n",
        "\n",
        "# Check specific conversions\n",
        "expected_conversions = [\n",
        "    [\"O\", \"O\"],\n",
        "    [\"B-LOC\", \"I-LOC\", \"E-LOC\"],\n",
        "    [\"S-LANG\"]\n",
        "]\n",
        "conversion_accuracy = all(\n",
        "    tags == expected\n",
        "    for (_, tags), expected in zip(converted_dataset, expected_conversions)\n",
        ")\n",
        "\n",
        "# Print results\n",
        "print(\"\\nIndividual Conversion Tests:\")\n",
        "print(f\"Test 1: Basic multi-token: {test1_success}\")\n",
        "print(f\"Test 2: Single-token entity: {test2_success}\")\n",
        "print(f\"Test 3: I at sequence end: {test3_success}\")\n",
        "print(f\"Test 4: Inconsistent tags: {test4_success} (check warnings)\")\n",
        "print(f\"Test 5: Isolated I tag: {test5_success} (check warnings)\")\n",
        "print(f\"Test 6: Mixed entities: {test6_success}\")\n",
        "print(f\"Test 7: All O tags: {test7_success}\")\n",
        "\n",
        "print(\"\\nDataset Conversion Tests:\")\n",
        "print(f\"Output is list: {is_dataset_list}\")\n",
        "print(f\"All items are tuples: {all_tuples}\")\n",
        "print(f\"All word/tag lengths match: {all_lengths_match}\")\n",
        "print(f\"All tags valid IOBES (exact): {valid_tags}\")\n",
        "print(f\"All tags valid IOBES (general): {valid_tags_general}\")\n",
        "print(f\"Final accuracy check: {conversion_accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7XI4CIuFRk1w",
        "outputId": "fdf27683-8670-4b7c-bfd9-f44430e7236c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Inconsistent tag sequence found: B-X\n",
            "                    followed by I-Y. Converting B-X to S-X.\n",
            "\n",
            "Individual Conversion Tests:\n",
            "Test 1: Basic multi-token: True\n",
            "Test 2: Single-token entity: True\n",
            "Test 3: I at sequence end: True\n",
            "Test 4: Inconsistent tags: False (check warnings)\n",
            "Test 5: Isolated I tag: False (check warnings)\n",
            "Test 6: Mixed entities: True\n",
            "Test 7: All O tags: True\n",
            "\n",
            "Dataset Conversion Tests:\n",
            "Output is list: True\n",
            "All items are tuples: True\n",
            "All word/tag lengths match: True\n",
            "All tags valid IOBES (exact): True\n",
            "All tags valid IOBES (general): True\n",
            "Final accuracy check: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert datasets to IOBES\n",
        "train_iobes = convert_dataset_to_iobes(train)\n",
        "dev_iobes = convert_dataset_to_iobes(dev)\n",
        "test_iobes = convert_dataset_to_iobes(test)"
      ],
      "metadata": {
        "id": "RFOZ1TMLFXpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display example of conversion\n",
        "print(\"Example of IOB to IOBES conversion:\")\n",
        "example_idx = 0  # Pick the first sentence as an example\n",
        "print(\"Original (IOB):\", train[example_idx][1][:10])  # Show first 10 tags\n",
        "print(\"Converted (IOBES):\", train_iobes[example_idx][1][:10])"
      ],
      "metadata": {
        "id": "LplGVZcWFZkR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e3ef701-a875-44c6-8e88-ae73e262e182"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example of IOB to IOBES conversion:\n",
            "Original (IOB): ['B-ORG', 'O', 'B-ORG', 'O']\n",
            "Converted (IOBES): ['S-ORG', 'O', 'S-ORG', 'O']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuGwk6OwRWGS"
      },
      "source": [
        "<!-- ## Step 3: Create vocab\n",
        "\n",
        "The following `Vocab` class can be served as a dictionary that maps words and tags into Ids.   \n",
        "The `UNK_TOKEN` should be used for words that are not part of the training data.\n",
        "\n",
        "Note: you may change the Vocab class. -->\n",
        "\n",
        "## Step 3: Create Vocab\n",
        "\n",
        "The `Vocab` class will serve as a dictionary that maps words and tags into IDs. Use the `UNK_TOKEN` for words that are not part of the training data.\n",
        "\n",
        "**Special Tokens:**\n",
        "- `PAD_TOKEN = 0`\n",
        "- `UNK_TOKEN = 1`\n",
        "\n",
        "### Your Task\n",
        "1. **Define Special Tokens**: Define special tokens such as `PAD_TOKEN` and `UNK_TOKEN` and assign them unique IDs.\n",
        "2. **Initialize Dictionaries**: Populate the word and tag dictionaries based on the training set.\n",
        "\n",
        "Note: you may change the Vocab class."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initinize ids for special tokens\n",
        "PAD_TOKEN = 0\n",
        "UNK_TOKEN = 1\n",
        "\n",
        "class Vocab:\n",
        "  def __init__(self, train: DataType):\n",
        "    \"\"\"\n",
        "    Initialize a Vocab instance.\n",
        "    :param train: train data\n",
        "    \"\"\"\n",
        "    self.word2id = {\"__unk__\": UNK_TOKEN, \"__pad__\": PAD_TOKEN}\n",
        "    self.id2word = {UNK_TOKEN: \"__unk__\", PAD_TOKEN: \"__pad__\"}\n",
        "    self.n_words = 2\n",
        "\n",
        "    self.tag2id = {}\n",
        "    self.id2tag = {}\n",
        "    self.n_tags = 0\n",
        "\n",
        "    # Initialize dictionaries based on the training set\n",
        "    # TO DO ----------------------------------------------------------------------\n",
        "\n",
        "    # init the dictionary for words\n",
        "    all_words = [word for sentence_pair in train for word in sentence_pair[0]]\n",
        "    unique_words = sorted(list(set(all_words)))\n",
        "    for i, word in enumerate(unique_words):\n",
        "      if word not in self.word2id: # Avoid re-adding special tokens if they appear in data\n",
        "          self.word2id[word] = self.n_words\n",
        "          self.id2word[self.n_words] = word\n",
        "          self.n_words += 1\n",
        "\n",
        "    # init the dictionary for tags\n",
        "    all_tags = [tag for sentence_pair in train for tag in sentence_pair[1]]\n",
        "    unique_tags = sorted(list(set(all_tags)))\n",
        "    for i, tag in enumerate(unique_tags):\n",
        "        self.tag2id[tag] = self.n_tags\n",
        "        self.id2tag[self.n_tags] = tag\n",
        "        self.n_tags += 1\n",
        "\n",
        "    # verifies that we have all the iobes tags in the dataset\n",
        "    EXPECTED_TAGS = {\n",
        "    'B-LOC', 'B-ORG', 'B-PER',\n",
        "    'I-LOC', 'I-ORG', 'I-PER',\n",
        "    'E-LOC', 'E-ORG', 'E-PER',\n",
        "    'S-LOC', 'S-ORG', 'S-PER',\n",
        "    'O'}\n",
        "\n",
        "    missing_tags = EXPECTED_TAGS - set(self.tag2id.keys())\n",
        "    if missing_tags:\n",
        "      print(f\"Missing expected tags in vocabulary: {missing_tags}\")\n",
        "\n",
        "    # verifies that we have only IOBES tags\n",
        "    actual_tags = set(self.tag2id.keys())\n",
        "    unexpected_tags = actual_tags - EXPECTED_TAGS\n",
        "    if unexpected_tags:\n",
        "        print(f\"‚ö†Ô∏è Non-standard tags found in data: {unexpected_tags}\")\n",
        "\n",
        "    # TO DO ----------------------------------------------------------------------\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.n_words\n",
        "\n",
        "  def index_tags(self, tags: list[str]) -> list[int]:\n",
        "    \"\"\"\n",
        "    Convert tags to Ids.\n",
        "    :param tags: list of tags\n",
        "    :return: list of Ids\n",
        "    \"\"\"\n",
        "    tag_indexes = [self.tag2id[t] for t in tags]\n",
        "    return tag_indexes\n",
        "\n",
        "  def index_words(self, words: list[str]) -> list[int]:\n",
        "    \"\"\"\n",
        "    Convert words to Ids.\n",
        "    :param words: list of words\n",
        "    :return: list of Ids\n",
        "    \"\"\"\n",
        "    word_indexes = [self.word2id[w] if w in self.word2id else self.word2id[\"__unk__\"] for w in words]\n",
        "    return word_indexes"
      ],
      "metadata": {
        "id": "IFQUUJimE2QG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################################## Vocab Tests ###################################\n",
        "\n",
        "# Sample training data\n",
        "train_data = [\n",
        "    ([\"Hello\", \"world\"], [\"O\", \"O\"]),\n",
        "    ([\"New\", \"York\", \"City\"], [\"B-LOC\", \"I-LOC\", \"I-LOC\"]),\n",
        "    ([\"Python\"], [\"B-LANG\"])\n",
        "]\n",
        "\n",
        "# Initialize vocabulary\n",
        "vocab = Vocab(train_data)\n",
        "\n",
        "# Structural tests\n",
        "is_vocab_instance = isinstance(vocab, Vocab)\n",
        "word_dict_is_dict = isinstance(vocab.word2id, dict)\n",
        "tag_dict_is_dict = isinstance(vocab.tag2id, dict)\n",
        "\n",
        "# Special token checks\n",
        "has_unk_pad = \"__unk__\" in vocab.word2id and \"__pad__\" in vocab.word2id\n",
        "unk_correct_id = vocab.word2id[\"__unk__\"] == UNK_TOKEN\n",
        "pad_correct_id = vocab.word2id[\"__pad__\"] == PAD_TOKEN\n",
        "\n",
        "# Word coverage tests\n",
        "all_train_words = {word for sentence in train_data for word in sentence[0]}\n",
        "all_words_in_vocab = all(word in vocab.word2id for word in all_train_words)\n",
        "unique_word_count = len(all_train_words) + 2  # +2 for special tokens\n",
        "word_count_matches = vocab.n_words == unique_word_count\n",
        "\n",
        "# Tag coverage tests\n",
        "all_train_tags = {tag for sentence in train_data for tag in sentence[1]}\n",
        "all_tags_in_vocab = all(tag in vocab.tag2id for tag in all_train_tags)\n",
        "unique_tag_count = len(all_train_tags)\n",
        "tag_count_matches = vocab.n_tags == unique_tag_count\n",
        "\n",
        "# Indexing tests\n",
        "test_words = [\"Hello\", \"unknown_word\", \"Python\"]\n",
        "expected_word_ids = [\n",
        "    vocab.word2id[\"Hello\"],\n",
        "    UNK_TOKEN,\n",
        "    vocab.word2id[\"Python\"]\n",
        "]\n",
        "word_indexing_correct = vocab.index_words(test_words) == expected_word_ids\n",
        "\n",
        "test_tags = [\"O\", \"B-LOC\", \"I-LOC\"]\n",
        "expected_tag_ids = [\n",
        "    vocab.tag2id[\"O\"],\n",
        "    vocab.tag2id[\"B-LOC\"],\n",
        "    vocab.tag2id[\"I-LOC\"]\n",
        "]\n",
        "tag_indexing_correct = vocab.index_tags(test_tags) == expected_tag_ids\n",
        "\n",
        "# Print results\n",
        "print(\"Structural Tests:\")\n",
        "print(f\"Test: Vocab initialized correctly: {is_vocab_instance}\")\n",
        "print(f\"Test: word2id is dict: {word_dict_is_dict}\")\n",
        "print(f\"Test: tag2id is dict: {tag_dict_is_dict}\")\n",
        "\n",
        "print(\"\\nSpecial Token Tests:\")\n",
        "print(f\"Test: Contains UNK/PAD: {has_unk_pad}\")\n",
        "print(f\"Test: UNK has ID 1: {unk_correct_id}\")\n",
        "print(f\"Test: PAD has ID 0: {pad_correct_id}\")\n",
        "\n",
        "print(\"\\nWord Coverage Tests:\")\n",
        "print(f\"Test: All training words in vocab: {all_words_in_vocab}\")\n",
        "print(f\"Test: Word count matches (expected {unique_word_count}): {word_count_matches}\")\n",
        "\n",
        "print(\"\\nTag Coverage Tests:\")\n",
        "print(f\"Test: All training tags in vocab: {all_tags_in_vocab}\")\n",
        "print(f\"Test: Tag count matches (expected {unique_tag_count}): {tag_count_matches}\")\n",
        "\n",
        "print(\"\\nIndexing Tests:\")\n",
        "print(f\"Test: Word indexing correct: {word_indexing_correct}\")\n",
        "print(f\"Test: Tag indexing correct: {tag_indexing_correct}\")\n",
        "\n",
        "# Verify tag ID consistency\n",
        "print(\"\\nTag ID Consistency Check:\")\n",
        "print(f\"O -> {vocab.tag2id['O']} (should be 0)\")\n",
        "print(f\"B-LOC -> {vocab.tag2id['B-LOC']} (should be 1)\")\n",
        "print(f\"I-LOC -> {vocab.tag2id['I-LOC']} (should be 2)\")\n",
        "print(f\"B-LANG -> {vocab.tag2id['B-LANG']} (should be 3)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u84QAn9RTLB5",
        "outputId": "a58ace4b-f657-4f20-efb3-fe6f0f93d062"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing expected tags in vocabulary: {'I-PER', 'I-ORG', 'E-ORG', 'E-LOC', 'B-ORG', 'B-PER', 'S-LOC', 'E-PER', 'S-ORG', 'S-PER'}\n",
            "‚ö†Ô∏è Non-standard tags found in data: {'B-LANG'}\n",
            "Structural Tests:\n",
            "Test: Vocab initialized correctly: True\n",
            "Test: word2id is dict: True\n",
            "Test: tag2id is dict: True\n",
            "\n",
            "Special Token Tests:\n",
            "Test: Contains UNK/PAD: True\n",
            "Test: UNK has ID 1: True\n",
            "Test: PAD has ID 0: True\n",
            "\n",
            "Word Coverage Tests:\n",
            "Test: All training words in vocab: True\n",
            "Test: Word count matches (expected 8): True\n",
            "\n",
            "Tag Coverage Tests:\n",
            "Test: All training tags in vocab: True\n",
            "Test: Tag count matches (expected 4): True\n",
            "\n",
            "Indexing Tests:\n",
            "Test: Word indexing correct: True\n",
            "Test: Tag indexing correct: True\n",
            "\n",
            "Tag ID Consistency Check:\n",
            "O -> 3 (should be 0)\n",
            "B-LOC -> 1 (should be 1)\n",
            "I-LOC -> 2 (should be 2)\n",
            "B-LANG -> 0 (should be 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0H2D2zSR4GQ9",
        "outputId": "3c3919e3-38db-4417-b11b-05ed5c81091f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'B-LOC': 0, 'B-ORG': 1, 'B-PER': 2, 'E-LOC': 3, 'E-ORG': 4, 'E-PER': 5, 'I-LOC': 6, 'I-ORG': 7, 'I-PER': 8, 'O': 9, 'S-LOC': 10, 'S-ORG': 11, 'S-PER': 12}\n",
            "Vocab size: 7163\n"
          ]
        }
      ],
      "source": [
        "vocab = Vocab(train_iobes)\n",
        "print(vocab.tag2id)\n",
        "print(f\"Vocab size: {vocab.n_words}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDKYryfKfNdh"
      },
      "source": [
        "## Step 4: Prepare Data\n",
        "Write a function `prepare_data` that takes one of the [train, dev, test] and the `Vocab` instance, for converting each pair of (words, tags) to a pair of indexes. Additionally, the function should pad the sequences to the maximum length sequence **of the given split**.\n",
        "\n",
        "Note: Vocabulary is based only on the train set.\n",
        "\n",
        "### Your Task\n",
        "1. Convert each pair of (words, tags) to a pair of indexes using the Vocab instance.\n",
        "2. Pad the sequences to the maximum length of the sequences in the given split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "noIY3zWKvhBd"
      },
      "outputs": [],
      "source": [
        "def prepare_data(data: DataType, vocab: Vocab):\n",
        "  data_sequences = []\n",
        "  # TO DO ----------------------------------------------------------------------\n",
        "\n",
        "  if not data:\n",
        "        return []\n",
        "\n",
        "  max_length = max(len(sentence[0]) for sentence in data)\n",
        "\n",
        "  for words, tags in data:\n",
        "      # Cconvert words to IDs\n",
        "      word_ids = vocab.index_words(words)\n",
        "\n",
        "      # convert tags to IDs\n",
        "      tag_ids = vocab.index_tags(tags)\n",
        "\n",
        "      # check how many pads are required\n",
        "      required_pads = max_length - len(word_ids)\n",
        "\n",
        "      # do the padding for words ids\n",
        "      padded_word_ids = word_ids + [PAD_TOKEN] * required_pads\n",
        "\n",
        "      # do the padding for tags\n",
        "      padded_tag_ids = tag_ids + [PAD_TOKEN] * required_pads\n",
        "\n",
        "      # add the padded sequences to data_sequences\n",
        "      data_sequences.append((padded_word_ids, padded_tag_ids))\n",
        "\n",
        "  # TO DO ----------------------------------------------------------------------\n",
        "  return data_sequences"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "############################# prepare_data Tests ###############################\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "#  Sample vocabulary (built only from \"train\" sentences, as required)\n",
        "# ------------------------------------------------------------------\n",
        "sample_vocab = Vocab([\n",
        "    ([\"Hello\", \"world\"], [\"O\", \"O\"]),\n",
        "    ([\"New\", \"York\"],     [\"B-LOC\", \"I-LOC\"]),\n",
        "    ([\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\"], [\"O\", \"O\", \"B-X\", \"B-Y\", \"I-Y\", \"I-Y\", \"B-Z\"])\n",
        "])\n",
        "\n",
        "# --------------------------\n",
        "#  Test 1 ‚Äì basic padding\n",
        "# --------------------------\n",
        "test1_data = [\n",
        "    ([\"Hello\", \"world\"],            [\"O\", \"O\"]),\n",
        "    ([\"New\", \"York\", \"City\"],       [\"B-LOC\", \"I-LOC\", \"I-LOC\"])   # ‚ÄúCity‚Äù is unknown\n",
        "]\n",
        "test1_expected_len = 3\n",
        "test1_out   = prepare_data(test1_data, sample_vocab)\n",
        "test1_pass  = all(len(w)==test1_expected_len and len(t)==test1_expected_len\n",
        "                  for w,t in test1_out)\n",
        "\n",
        "# --------------------------\n",
        "#  Test 2 ‚Äì unknown word\n",
        "# --------------------------\n",
        "#  Only one sentence ‚Üí max_len = 1 ‚Üí **no padding needed**\n",
        "test2_input  = ([\"Python3\"], [\"O\"])                 # ‚ÄúPython3‚Äù is unknown\n",
        "test2_out    = prepare_data([test2_input], sample_vocab)\n",
        "test2_pass   = test2_out[0][0] == [UNK_TOKEN]       # expect just UNK_TOKEN\n",
        "\n",
        "# --------------------------\n",
        "#  Test 3 ‚Äì empty split\n",
        "# --------------------------\n",
        "test3_out   = prepare_data([], sample_vocab)\n",
        "test3_pass  = test3_out == []\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "#  Test 4 ‚Äì padding values (create a split where padding is needed)\n",
        "# ---------------------------------------------------------------\n",
        "test4_data = [\n",
        "    ([\"Hello\"],           [\"O\"]),                   # will need padding\n",
        "    ([\"New\", \"York\"],     [\"B-LOC\", \"I-LOC\"])       # longest ‚áí max_len = 2\n",
        "]\n",
        "test4_out   = prepare_data(test4_data, sample_vocab)\n",
        "hello_id    = sample_vocab.word2id[\"Hello\"]\n",
        "o_tag_id    = sample_vocab.tag2id[\"O\"]\n",
        "\n",
        "test4_pass = (\n",
        "    test4_out[0][0] == [hello_id, PAD_TOKEN] and    # words padded\n",
        "    test4_out[0][1] == [o_tag_id,  PAD_TOKEN]       # tags padded\n",
        ")\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "#  Dataset-level consistency checks (train/dev/test toy splits)\n",
        "# ------------------------------------------------------------------\n",
        "train_data = [([\"a\", \"b\"], [\"O\", \"O\"]), ([\"c\"], [\"B-X\"])]\n",
        "dev_data   = [([\"d\", \"e\", \"f\"], [\"B-Y\", \"I-Y\", \"I-Y\"])]\n",
        "test_data  = [([\"g\"], [\"B-Z\"])]\n",
        "\n",
        "train_seq = prepare_data(train_data, sample_vocab)\n",
        "dev_seq   = prepare_data(dev_data,   sample_vocab)\n",
        "test_seq  = prepare_data(test_data,  sample_vocab)\n",
        "\n",
        "def split_ok(seqs, raw):\n",
        "    exp_len = max(len(s[0]) for s in raw)\n",
        "    same    = all(len(w)==exp_len and len(t)==exp_len for w,t in seqs)\n",
        "    w_ids   = all(0 <= idx < sample_vocab.n_words for w,_ in seqs for idx in w)\n",
        "    t_ids   = all(idx in sample_vocab.tag2id.values() or idx==PAD_TOKEN\n",
        "                  for _,t in seqs for idx in t)\n",
        "    return same and w_ids and t_ids\n",
        "\n",
        "dataset_pass = {\n",
        "    \"Train\": split_ok(train_seq, train_data),\n",
        "    \"Dev\"  : split_ok(dev_seq,   dev_data),\n",
        "    \"Test\" : split_ok(test_seq,  test_data),\n",
        "}\n",
        "\n",
        "# -------------------\n",
        "#  Print test report\n",
        "# -------------------\n",
        "print(\"=========== prepare_data Unit Tests ===========\")\n",
        "print(f\"Test 1 ‚Äì basic padding ............... {test1_pass}\")\n",
        "print(f\"Test 2 ‚Äì unknown word ................ {test2_pass}\")\n",
        "print(f\"Test 3 ‚Äì empty split ................. {test3_pass}\")\n",
        "print(f\"Test 4 ‚Äì padding values .............. {test4_pass}\")\n",
        "\n",
        "print(\"\\n=========== Dataset Consistency ===========\")\n",
        "for name, ok in dataset_pass.items():\n",
        "    print(f\"{name:5s} split ........................ {ok}\")\n",
        "\n",
        "# Optional: extra diagnostics when something fails\n",
        "if not test2_pass:\n",
        "    print(\"  ‚Ü≥ got word IDs:\", test2_out[0][0])\n",
        "if not test4_pass:\n",
        "    print(\"  ‚Ü≥ got:\", test4_out[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTED5disbAYP",
        "outputId": "6fd3376a-b9da-47e5-b685-60ab07846e05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing expected tags in vocabulary: {'I-PER', 'I-ORG', 'E-ORG', 'E-LOC', 'B-ORG', 'B-PER', 'S-LOC', 'E-PER', 'S-ORG', 'S-PER'}\n",
            "‚ö†Ô∏è Non-standard tags found in data: {'B-Y', 'B-Z', 'B-X', 'I-Y'}\n",
            "=========== prepare_data Unit Tests ===========\n",
            "Test 1 ‚Äì basic padding ............... True\n",
            "Test 2 ‚Äì unknown word ................ True\n",
            "Test 3 ‚Äì empty split ................. True\n",
            "Test 4 ‚Äì padding values .............. True\n",
            "\n",
            "=========== Dataset Consistency ===========\n",
            "Train split ........................ True\n",
            "Dev   split ........................ True\n",
            "Test  split ........................ True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4mryWuq6rox"
      },
      "outputs": [],
      "source": [
        "train_sequences = prepare_data(train_iobes, vocab)\n",
        "dev_sequences = prepare_data(dev_iobes, vocab)\n",
        "test_sequences = prepare_data(test_iobes, vocab)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSR-qJnIQGUJ"
      },
      "source": [
        "### Your Task\n",
        "Print the number of OOV in dev and test sets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QaXaJe5wrGhl"
      },
      "outputs": [],
      "source": [
        "def count_oov(sequences) -> int:\n",
        "  \"\"\"\n",
        "  Count the number of OOV words.\n",
        "  :param sequences: list of sequences\n",
        "  :return: number of OOV words\n",
        "  \"\"\"\n",
        "  oov = -1\n",
        "  # TO DO ----------------------------------------------------------------------\n",
        "\n",
        "  # iterate over all pair of words-tags\n",
        "  for word_ids, tag_ids in sequences:\n",
        "    # iterate through the word IDs in the current sentence\n",
        "    for word_id in word_ids:\n",
        "      # count word if it is out of vocabulary and not a padding\n",
        "      if word_id == UNK_TOKEN and word_id != PAD_TOKEN:\n",
        "        oov_count += 1\n",
        "\n",
        "  # TO DO ----------------------------------------------------------------------\n",
        "  return oov"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Extract Casing Features\n",
        "\n",
        "The casing of words (uppercase, lowercase, etc.) can provide useful information for NER. Let's add this feature to improve our model.\n",
        "\n",
        "### Your Task\n",
        "1. Implement a function to extract casing features for each word.\n",
        "2. Modify the data preparation to include these features.\n",
        "\n",
        "Casing features to extract:\n",
        "- Is all lowercase\n",
        "- Is all uppercase\n",
        "- Is titlecase (first letter capital)\n",
        "- Has non-initial capital letters\n",
        "- Contains digits\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "W-nWrqbLNZPY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_casing_features(word: str) -> list[int]:\n",
        "  \"\"\"\n",
        "  Extract casing features for a word.\n",
        "\n",
        "  Features:\n",
        "  - Is all lowercase (1/0)\n",
        "  - Is all uppercase (1/0)\n",
        "  - Is titlecase (first letter capital only) (1/0)\n",
        "  - Has non-initial capital letters (1/0)\n",
        "  - Contains digits (1/0)\n",
        "\n",
        "  Args:\n",
        "      word (str): Input word\n",
        "\n",
        "  Returns:\n",
        "      list[int]: Binary features [0/1, 0/1, 0/1, 0/1, 0/1]\n",
        "  \"\"\"\n",
        "  # TO DO ----------------------------------------------------------------------\n",
        "  features = [0] * 5\n",
        "\n",
        "\n",
        "  # is all lowercase\n",
        "  if word.islower():\n",
        "      features[0] = 1\n",
        "\n",
        "  # is all uppercase\n",
        "  if word.isupper():\n",
        "      features[1] = 1\n",
        "\n",
        "  # is titlecase\n",
        "  if word[0].isupper() and word[1:].islower():\n",
        "      features[2] = 1\n",
        "\n",
        "  # does have non-initial capital letters\n",
        "  if len(word) > 1:\n",
        "      for char in word[1:]:\n",
        "          if char.isupper():\n",
        "              features[3] = 1\n",
        "              break # Found a non-initial capital, no need to check further\n",
        "\n",
        "  # does contain digits\n",
        "  for char in word:\n",
        "      if char.isdigit():\n",
        "          features[4] = 1\n",
        "          break\n",
        "\n",
        "  # TO DO ----------------------------------------------------------------------\n",
        "  return features  # Placeholder\n",
        "\n",
        "def prepare_data_with_casing(data: DataType, vocab: Vocab):\n",
        "  \"\"\"\n",
        "  Prepare data with additional casing features.\n",
        "\n",
        "  Args:\n",
        "      data (DataType): Input data\n",
        "      vocab (Vocab): Vocabulary instance\n",
        "\n",
        "  Returns:\n",
        "      list: List of sequences with word IDs, casing features, and tag IDs\n",
        "  \"\"\"\n",
        "  data_sequences = []\n",
        "  # TO DO ----------------------------------------------------------------------\n",
        "\n",
        "  # taking care max length for padding\n",
        "  max_length = 0\n",
        "  if data:\n",
        "      max_length = max(len(sentence[0]) for sentence in data)\n",
        "\n",
        "  # init padding_feature_vector for additional features\n",
        "  padding_feature_vector = [0] * 5\n",
        "\n",
        "  for words, tags in data:\n",
        "      # cconvert words to IDs\n",
        "      word_ids = vocab.index_words(words)\n",
        "\n",
        "      # convert tags to IDs\n",
        "      tag_ids = vocab.index_tags(tags)\n",
        "\n",
        "      # check how many pads are required\n",
        "      required_pads = max_length - len(words)\n",
        "\n",
        "      # extract casing features with the get_casing_features\n",
        "      casing_features_list = [get_casing_features(word) for word in words]\n",
        "\n",
        "      # do the padding for words ids\n",
        "      padded_word_ids = word_ids + [PAD_TOKEN] * required_pads\n",
        "\n",
        "      # do the padding for tags\n",
        "      padded_tag_ids = tag_ids + [PAD_TOKEN] * required_pads\n",
        "\n",
        "      # pad the list of casing feature vectors with the padding feature vector\n",
        "      padded_casing_features = casing_features_list + [padding_feature_vector] * required_pads\n",
        "\n",
        "      # add the padded sequences to data_sequences including additional dimension for casing features\n",
        "      data_sequences.append((padded_word_ids, padded_casing_features, padded_tag_ids))\n",
        "\n",
        "  # TO DO ----------------------------------------------------------------------\n",
        "  return data_sequences"
      ],
      "metadata": {
        "id": "mTihlirsVBvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data with casing features\n",
        "train_sequences_casing = prepare_data_with_casing(train_iobes, vocab)\n",
        "dev_sequences_casing = prepare_data_with_casing(dev_iobes, vocab)\n",
        "test_sequences_casing = prepare_data_with_casing(test_iobes, vocab)"
      ],
      "metadata": {
        "id": "9vbsCMTjNpdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Utility function to print nicely formatted section headers\n",
        "def section(title):\n",
        "    print(\"\\n\" + \"=\"*len(title))\n",
        "    print(title)\n",
        "    print(\"=\"*len(title))\n",
        "\n",
        "# Function to show detailed pipeline for a single sentence from a split\n",
        "def trace_pipeline(name: str, dataset: DataType, vocab: Vocab):\n",
        "    section(f\"EXAMPLE FROM {name.upper()} DATASET\")\n",
        "\n",
        "    idx = random.randint(0, len(dataset)-1)\n",
        "    words, tags = dataset[idx]\n",
        "\n",
        "    print(f\"\\nüëâ Original Sentence (raw):\\n{words}\")\n",
        "    print(f\"üëâ Original Tags (IOB):\\n{tags}\")\n",
        "\n",
        "    # Step 1: IOB ‚ûú IOBES\n",
        "    converted_tags = iob_to_iobes(tags)\n",
        "    print(f\"\\n‚úÖ After iob_to_iobes:\\n{converted_tags}\")\n",
        "\n",
        "    # Step 2: Apply vocab indexing\n",
        "    word_ids = vocab.index_words(words)\n",
        "    tag_ids = vocab.index_tags(converted_tags)\n",
        "    print(f\"\\n‚úÖ Word IDs:\\n{word_ids}\")\n",
        "    print(f\"‚úÖ Tag IDs:\\n{tag_ids}\")\n",
        "\n",
        "    # Step 3: Casing features\n",
        "    casing_features = [get_casing_features(w) for w in words]\n",
        "    print(f\"\\n‚úÖ Casing Features (before padding):\\n{casing_features}\")\n",
        "\n",
        "    # Step 4: Padding (as in prepare_data_with_casing)\n",
        "    max_len = len(words)  # local max for just this sentence\n",
        "    padding_len = 0  # we are doing single sentence, so no real padding needed here\n",
        "    pad_feat = [0]*5\n",
        "    padded_word_ids = word_ids + [PAD_TOKEN] * padding_len\n",
        "    padded_tag_ids = tag_ids + [PAD_TOKEN] * padding_len\n",
        "    padded_feats = casing_features + [pad_feat] * padding_len\n",
        "\n",
        "    print(f\"\\n‚úÖ Final Output (padded, prepare_data_with_casing style):\")\n",
        "    print(f\"Words      : {words}\")\n",
        "    print(f\"Word IDs   : {padded_word_ids}\")\n",
        "    print(f\"Features   : {padded_feats}\")\n",
        "    print(f\"Tag IDs    : {padded_tag_ids}\")\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "# Initialize full pipeline trace for one sentence per dataset\n",
        "# --------------------------------------------------------------\n",
        "trace_pipeline(\"train\", train_iobes, vocab)\n",
        "trace_pipeline(\"dev\", dev_iobes, vocab)\n",
        "trace_pipeline(\"test\", test_iobes, vocab)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dmibnrvdd6vD",
        "outputId": "28498289-7709-411e-eb35-d9ac0d59b5cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==========================\n",
            "EXAMPLE FROM TRAIN DATASET\n",
            "==========================\n",
            "\n",
            "üëâ Original Sentence (raw):\n",
            "['20.', 'Giovanni', 'Lavaggi', '(', 'Italy', ')', 'Minardi', '1:58.579']\n",
            "üëâ Original Tags (IOB):\n",
            "['O', 'B-PER', 'E-PER', 'O', 'S-LOC', 'O', 'S-ORG', 'O']\n",
            "Warning: Encountered an unexpected or isolated tag format: E-PER. Treating as 'O'.\n",
            "Warning: Encountered an unexpected or isolated tag format: S-LOC. Treating as 'O'.\n",
            "Warning: Encountered an unexpected or isolated tag format: S-ORG. Treating as 'O'.\n",
            "\n",
            "‚úÖ After iob_to_iobes:\n",
            "['O', 'S-PER', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "\n",
            "‚úÖ Word IDs:\n",
            "[386, 1891, 2441, 11, 2191, 12, 2681, 366]\n",
            "‚úÖ Tag IDs:\n",
            "[9, 12, 9, 9, 9, 9, 9, 9]\n",
            "\n",
            "‚úÖ Casing Features (before padding):\n",
            "[[0, 0, 0, 0, 1], [0, 0, 1, 0, 0], [0, 0, 1, 0, 0], [0, 0, 0, 0, 0], [0, 0, 1, 0, 0], [0, 0, 0, 0, 0], [0, 0, 1, 0, 0], [0, 0, 0, 0, 1]]\n",
            "\n",
            "‚úÖ Final Output (padded, prepare_data_with_casing style):\n",
            "Words      : ['20.', 'Giovanni', 'Lavaggi', '(', 'Italy', ')', 'Minardi', '1:58.579']\n",
            "Word IDs   : [386, 1891, 2441, 11, 2191, 12, 2681, 366]\n",
            "Features   : [[0, 0, 0, 0, 1], [0, 0, 1, 0, 0], [0, 0, 1, 0, 0], [0, 0, 0, 0, 0], [0, 0, 1, 0, 0], [0, 0, 0, 0, 0], [0, 0, 1, 0, 0], [0, 0, 0, 0, 1]]\n",
            "Tag IDs    : [9, 12, 9, 9, 9, 9, 9, 9]\n",
            "\n",
            "========================\n",
            "EXAMPLE FROM DEV DATASET\n",
            "========================\n",
            "\n",
            "üëâ Original Sentence (raw):\n",
            "['To', 'bat', '-', 'Inzamam-ul-Haq', ',', 'Salim', 'Malik', ',', 'Asif', 'Mujtaba', ',', 'Wasim', 'Akram', ',', 'Moin', 'Khan', ',', 'Mushtaq', 'Ahmed', ',', 'Waqar', 'Younis', ',', 'Mohammad', 'Akam']\n",
            "üëâ Original Tags (IOB):\n",
            "['O', 'O', 'O', 'S-PER', 'O', 'B-PER', 'E-PER', 'O', 'B-PER', 'E-PER', 'O', 'B-PER', 'E-PER', 'O', 'B-PER', 'E-PER', 'O', 'B-PER', 'E-PER', 'O', 'B-PER', 'E-PER', 'O', 'B-PER', 'E-PER']\n",
            "Warning: Encountered an unexpected or isolated tag format: S-PER. Treating as 'O'.\n",
            "Warning: Encountered an unexpected or isolated tag format: E-PER. Treating as 'O'.\n",
            "Warning: Encountered an unexpected or isolated tag format: E-PER. Treating as 'O'.\n",
            "Warning: Encountered an unexpected or isolated tag format: E-PER. Treating as 'O'.\n",
            "Warning: Encountered an unexpected or isolated tag format: E-PER. Treating as 'O'.\n",
            "Warning: Encountered an unexpected or isolated tag format: E-PER. Treating as 'O'.\n",
            "Warning: Encountered an unexpected or isolated tag format: E-PER. Treating as 'O'.\n",
            "Warning: Encountered an unexpected or isolated tag format: E-PER. Treating as 'O'.\n",
            "\n",
            "‚úÖ After iob_to_iobes:\n",
            "['O', 'O', 'O', 'O', 'O', 'S-PER', 'O', 'O', 'S-PER', 'O', 'O', 'S-PER', 'O', 'O', 'S-PER', 'O', 'O', 'S-PER', 'O', 'O', 'S-PER', 'O', 'O', 'S-PER', 'O']\n",
            "\n",
            "‚úÖ Word IDs:\n",
            "[3675, 4245, 31, 2162, 30, 3311, 2570, 30, 1031, 2747, 30, 3863, 925, 30, 2709, 2344, 30, 2758, 918, 30, 3857, 3964, 30, 2707, 922]\n",
            "‚úÖ Tag IDs:\n",
            "[9, 9, 9, 9, 9, 12, 9, 9, 12, 9, 9, 12, 9, 9, 12, 9, 9, 12, 9, 9, 12, 9, 9, 12, 9]\n",
            "\n",
            "‚úÖ Casing Features (before padding):\n",
            "[[0, 0, 1, 0, 0], [1, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 1, 0], [0, 0, 0, 0, 0], [0, 0, 1, 0, 0], [0, 0, 1, 0, 0], [0, 0, 0, 0, 0], [0, 0, 1, 0, 0], [0, 0, 1, 0, 0], [0, 0, 0, 0, 0], [0, 0, 1, 0, 0], [0, 0, 1, 0, 0], [0, 0, 0, 0, 0], [0, 0, 1, 0, 0], [0, 0, 1, 0, 0], [0, 0, 0, 0, 0], [0, 0, 1, 0, 0], [0, 0, 1, 0, 0], [0, 0, 0, 0, 0], [0, 0, 1, 0, 0], [0, 0, 1, 0, 0], [0, 0, 0, 0, 0], [0, 0, 1, 0, 0], [0, 0, 1, 0, 0]]\n",
            "\n",
            "‚úÖ Final Output (padded, prepare_data_with_casing style):\n",
            "Words      : ['To', 'bat', '-', 'Inzamam-ul-Haq', ',', 'Salim', 'Malik', ',', 'Asif', 'Mujtaba', ',', 'Wasim', 'Akram', ',', 'Moin', 'Khan', ',', 'Mushtaq', 'Ahmed', ',', 'Waqar', 'Younis', ',', 'Mohammad', 'Akam']\n",
            "Word IDs   : [3675, 4245, 31, 2162, 30, 3311, 2570, 30, 1031, 2747, 30, 3863, 925, 30, 2709, 2344, 30, 2758, 918, 30, 3857, 3964, 30, 2707, 922]\n",
            "Features   : [[0, 0, 1, 0, 0], [1, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 1, 0], [0, 0, 0, 0, 0], [0, 0, 1, 0, 0], [0, 0, 1, 0, 0], [0, 0, 0, 0, 0], [0, 0, 1, 0, 0], [0, 0, 1, 0, 0], [0, 0, 0, 0, 0], [0, 0, 1, 0, 0], [0, 0, 1, 0, 0], [0, 0, 0, 0, 0], [0, 0, 1, 0, 0], [0, 0, 1, 0, 0], [0, 0, 0, 0, 0], [0, 0, 1, 0, 0], [0, 0, 1, 0, 0], [0, 0, 0, 0, 0], [0, 0, 1, 0, 0], [0, 0, 1, 0, 0], [0, 0, 0, 0, 0], [0, 0, 1, 0, 0], [0, 0, 1, 0, 0]]\n",
            "Tag IDs    : [9, 9, 9, 9, 9, 12, 9, 9, 12, 9, 9, 12, 9, 9, 12, 9, 9, 12, 9, 9, 12, 9, 9, 12, 9]\n",
            "\n",
            "=========================\n",
            "EXAMPLE FROM TEST DATASET\n",
            "=========================\n",
            "\n",
            "üëâ Original Sentence (raw):\n",
            "['Lincoln', '1', 'Leyton', 'Orient', '1']\n",
            "üëâ Original Tags (IOB):\n",
            "['S-ORG', 'O', 'B-ORG', 'E-ORG', 'O']\n",
            "Warning: Encountered an unexpected or isolated tag format: S-ORG. Treating as 'O'.\n",
            "Warning: Encountered an unexpected or isolated tag format: E-ORG. Treating as 'O'.\n",
            "\n",
            "‚úÖ After iob_to_iobes:\n",
            "['O', 'O', 'S-ORG', 'O', 'O']\n",
            "\n",
            "‚úÖ Word IDs:\n",
            "[1, 103, 1, 1, 103]\n",
            "‚úÖ Tag IDs:\n",
            "[9, 9, 11, 9, 9]\n",
            "\n",
            "‚úÖ Casing Features (before padding):\n",
            "[[0, 0, 1, 0, 0], [0, 0, 0, 0, 1], [0, 0, 1, 0, 0], [0, 0, 1, 0, 0], [0, 0, 0, 0, 1]]\n",
            "\n",
            "‚úÖ Final Output (padded, prepare_data_with_casing style):\n",
            "Words      : ['Lincoln', '1', 'Leyton', 'Orient', '1']\n",
            "Word IDs   : [1, 103, 1, 1, 103]\n",
            "Features   : [[0, 0, 1, 0, 0], [0, 0, 0, 0, 1], [0, 0, 1, 0, 0], [0, 0, 1, 0, 0], [0, 0, 0, 0, 1]]\n",
            "Tag IDs    : [9, 9, 11, 9, 9]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxC7SyD0EaMF"
      },
      "source": [
        "## Step 6: Dataloaders\n",
        "Create dataloaders for each split in the dataset. They should return the samples as Tensors.\n",
        "\n",
        "**Hint** - you can create a Dataset to support this part.\n",
        "\n",
        "For the training set, use shuffling, and for the dev and test, not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-zrosQU95vYZ"
      },
      "outputs": [],
      "source": [
        "# we will create costum dataset to handle the tensor conversion\n",
        "class DatasetNER(Dataset):\n",
        "    def __init__(self, sequences: list[tuple[list[int], list[int], list[int]]]):\n",
        "        self.sequences = sequences\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # print(self.sequences)\n",
        "        word_ids, casing_ids, tag_ids  = self.sequences[idx]\n",
        "        return (\n",
        "            th.tensor(word_ids, dtype=th.long),\n",
        "            th.tensor(casing_ids, dtype=th.long),\n",
        "            th.tensor(tag_ids, dtype=th.long)\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vpMZ1C15t4Ra"
      },
      "outputs": [],
      "source": [
        "def prepare_data_loader(sequences, batch_size: int, train: bool = True):\n",
        "  \"\"\"\n",
        "  Create a dataloader from a list of sequences.\n",
        "  :param sequences: list of sequences\n",
        "  :param batch_size: batch size\n",
        "  :param train: whether to shuffle the dataloader or not\n",
        "  :return: dataloader\n",
        "  \"\"\"\n",
        "  dataloader = None\n",
        "  # TO DO ----------------------------------------------------------------------\n",
        "  dataset = DatasetNER(sequences)\n",
        "  dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=train)\n",
        "  # TO DO ----------------------------------------------------------------------\n",
        "  return dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWhK-O-suA93",
        "outputId": "f2de076c-5d17-4887-e767-80255042e421"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(110, 16, 32)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "BATCH_SIZE = 16\n",
        "dl_train = prepare_data_loader(train_sequences_casing, batch_size=BATCH_SIZE)\n",
        "dl_dev = prepare_data_loader(dev_sequences_casing, batch_size=BATCH_SIZE, train=False)\n",
        "dl_test = prepare_data_loader(test_sequences_casing, batch_size=BATCH_SIZE, train=False)\n",
        "len(dl_train), len(dl_dev), len(dl_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeLdRU9R-pm3"
      },
      "source": [
        "<br><br><br><br><br><br>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for x_batch, casing_batch, y_batch in dl_train:\n",
        "    print(\"X shape:\", x_batch.shape)\n",
        "    print(\"Y shape:\", y_batch.shape)\n",
        "    print(\"Casing shape:\", casing_batch.shape)\n",
        "    # print(\"First X sample:\", x_batch[0])\n",
        "    # print(\"First Y sample:\", y_batch[0])\n",
        "    # print(\"First Casing sample:\", casing_batch[0])\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGmvzZ6lu5LH",
        "outputId": "d67a6a57-11f9-4f40-9723-2de727daadad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X shape: torch.Size([16, 58])\n",
            "Y shape: torch.Size([16, 58])\n",
            "Casing shape: torch.Size([16, 58, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUsgtdW869JH"
      },
      "source": [
        "# Part 2 - NER Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UccfiRRtiEet"
      },
      "source": [
        "## Step 1: Implement Model\n",
        "\n",
        "Write NERNet, a PyTorch Module for labeling words with NER tags.\n",
        "\n",
        "> `input_size`: the size of the vocabulary  \n",
        "`embedding_size`: the size of the embeddings  \n",
        "`hidden_size`: the LSTM hidden size  \n",
        "`output_size`: the number tags we are predicting for  \n",
        "`n_layers`: the number of layers we want to use in LSTM  \n",
        "`directions`: could 1 or 2, indicating unidirectional or bidirectional LSTM, respectively  \n",
        "\n",
        "<br>  \n",
        "\n",
        "The input for your forward function should be a single sentence tensor.\n",
        "\n",
        "*Note: the embeddings in this section are learned embedding. That means that you don't need to use pretrained embedding like the one used in the last excersie. You will use them in part 5.*\n",
        "\n",
        "*Note: You may change the NERNet class.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ke1LyUQNyQaM"
      },
      "outputs": [],
      "source": [
        "class NERNet(nn.Module):\n",
        "  def __init__(self, input_size: int, embedding_size: int, hidden_size: int, output_size: int, n_layers: int, directions: int):\n",
        "    \"\"\"\n",
        "    Initialize a NERNet instance.\n",
        "    :param input_size: the size of the vocabulary\n",
        "    :param embedding_size: the size of the embeddings\n",
        "    :param hidden_size: the LSTM hidden size\n",
        "    :param output_size: the number tags we are predicting for\n",
        "    :param n_layers: the number of layers we want to use in LSTM\n",
        "    :param directions: could be 1 or 2, indicating unidirectional or bidirectional LSTM, respectively\n",
        "    \"\"\"\n",
        "    super(NERNet, self).__init__()\n",
        "    # TO DO ----------------------------------------------------------------------\n",
        "    self.embedding = nn.Embedding(input_size, embedding_size, padding_idx=PAD_TOKEN)\n",
        "    self.lstm = nn.LSTM(input_size=embedding_size + 5,\n",
        "                        hidden_size=hidden_size,\n",
        "                        num_layers=n_layers,\n",
        "                        bidirectional=(directions == 2),\n",
        "                        batch_first=True)\n",
        "    self.classifier = nn.Linear(hidden_size * directions, output_size)\n",
        "\n",
        "\n",
        "    # TO DO ----------------------------------------------------------------------\n",
        "\n",
        "  def forward(self, input_sentence):\n",
        "    # TO DO ----------------------------------------------------------------------\n",
        "    # input_sentence: (batch_size, seq_len)\n",
        "\n",
        "    # unpack data from dataloader and ommits the tags\n",
        "    x_batch, casing_batch, _ = input_sentence\n",
        "\n",
        "    # embedds the one-hot encoding of the words (not the casing features) --- > [batch_size, seq_len, embedding_size]\n",
        "    embedded_words = self.embedding(x_batch)\n",
        "\n",
        "    # concat the casing features along the embedded words\n",
        "    lstm_input = th.cat([embedded_words, casing_batch], dim=-1)\n",
        "\n",
        "    # pass input throgh the lstm and ignore the hidden/cell state\n",
        "    output, _ = self.lstm(lstm_input)\n",
        "\n",
        "    # poroject the LSTM output to tag scores for each token\n",
        "    output = self.classifier(output)\n",
        "\n",
        "    # TO DO ----------------------------------------------------------------------\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0nrbHIei19s5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53935d72-21b6-44d6-cfaf-8fb2262a1120"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NERNet(\n",
              "  (embedding): Embedding(7163, 300, padding_idx=0)\n",
              "  (lstm): LSTM(305, 500, num_layers=3, batch_first=True, bidirectional=True)\n",
              "  (classifier): Linear(in_features=1000, out_features=13, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "model = NERNet(vocab.n_words, embedding_size=300, hidden_size=500, output_size=vocab.n_tags, n_layers=3, directions=2)\n",
        "model.to(DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEGSQdeUkTP8"
      },
      "source": [
        "## Step 2: Training Loop\n",
        "\n",
        "Write a training loop, which takes a model (instance of NERNet), number of epochs to train on, and the train&dev datasets.  \n",
        "\n",
        "The function will return the `loss` and `accuracy` durring training.  \n",
        "(If you're using a different/additional metrics, return them too)\n",
        "\n",
        "The loss is always CrossEntropyLoss and the optimizer is always Adam.\n",
        "Make sure to use `tqdm` while iterating on `n_epochs`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "avkHfjT3k0HM"
      },
      "outputs": [],
      "source": [
        "from tqdm import trange\n",
        "\n",
        "def train_loop(model: NERNet, n_epochs: int, dataloader_train, dataloader_dev):\n",
        "  \"\"\"\n",
        "  Train a model.\n",
        "  :param model: model instance\n",
        "  :param n_epochs: number of epochs to train on\n",
        "  :param dataloader_train: train dataloader\n",
        "  :param dataloader_dev: dev dataloader\n",
        "  :return: loss and accuracy during training\n",
        "  \"\"\"\n",
        "  # Optimizer (ADAM is a fancy version of SGD)\n",
        "  optimizer = Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "  # Record\n",
        "  metrics = {'loss': {'train': [], 'dev': []}, 'accuracy': {'train': [], 'dev': []}}\n",
        "\n",
        "  # Move model to device\n",
        "  model.to(DEVICE)\n",
        "\n",
        "  ## TO DO ----------------------------------------------------------------------\n",
        "\n",
        "  # defines the cross entropy as loss and specifies the padding token to prevent problems with the loss\n",
        "  loss_fn = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)\n",
        "\n",
        "\n",
        "  # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê Epoch Loop ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "  epoch_bar = trange(n_epochs, desc=\"Training\", leave=True)\n",
        "  for epoch in epoch_bar:\n",
        "  # for epoch in trange(n_epochs, desc=\"Training Epochs\"):\n",
        "\n",
        "    # set mode to training mode\n",
        "    model.train()\n",
        "\n",
        "    # cumulative loss, number of correctly predicted tokens, and total tokens for overall accuracy\n",
        "    total_loss, total_correct, total_tokens = 0, 0, 0\n",
        "\n",
        "    # to count how many predictions were correct for each tag\n",
        "    correct_per_class = {}\n",
        "    total_per_class = {}\n",
        "\n",
        "    # track gradient norms per parameteres\n",
        "    grad_norms = {}\n",
        "\n",
        "    # iterate over batches from data loader\n",
        "    for batch in dataloader_train:\n",
        "\n",
        "        # move batch data to the same device as the model to prevent gpu-cpu issues!\n",
        "        x_batch, casing_batch, y_batch = [x.to(DEVICE) for x in batch]\n",
        "\n",
        "        # reset gradients from the previous step to prevent unwanted accumulation\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward pass: run the batch through the model to get logits  (unnormalized tag scores)\n",
        "        outputs = model((x_batch, casing_batch, y_batch))  # shape: [batch_size, seq_len, num_tags]\n",
        "\n",
        "        # cross rntropy expect (N, C) where N is the number of samples and C is the number of classes\n",
        "        # reshape outputs and labels to match the expected input for CrossEntropyLoss:\n",
        "        # flatten both to (batch_size * seq_len, num_tags) and (batch_size * seq_len)\n",
        "        outputs = outputs.view(-1, outputs.shape[-1])  # logits: [batch_size*seq_len, num_tags]\n",
        "        targets = y_batch.view(-1)                     # targets: [batch_size*seq_len]\n",
        "\n",
        "        # compute the loss for the batch\n",
        "        loss = loss_fn(outputs, targets)\n",
        "\n",
        "        # backward pass: compute gradients with respect to model parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # optimizer step based on gradients\n",
        "        optimizer.step()\n",
        "\n",
        "        ############################ update metrics ############################\n",
        "        # accumulate total loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # compute predicted tags by selecting the index of the highest logit per token\n",
        "        predictions = outputs.argmax(dim=1)  # shape: [batch_size * seq_len]\n",
        "\n",
        "        # compute how many predictions were correct (ignore padding tokens)\n",
        "        mask = targets != PAD_TOKEN\n",
        "        correct = (predictions == targets) & mask\n",
        "        total_correct += correct.sum().item()\n",
        "        total_tokens += mask.sum().item()\n",
        "\n",
        "        # iterate over each (prediction, true label) pair atnon-padding positions only\n",
        "        for pred, true in zip(predictions[mask], targets[mask]):\n",
        "          # convert tensors to integer values represents the tag indices\n",
        "          true = true.item()\n",
        "          pred = pred.item()\n",
        "\n",
        "          # Update the total count for this true tag (if the tag is not already in the dictionary, initialize with 0)\n",
        "          total_per_class[true] = total_per_class.get(true, 0) + 1\n",
        "\n",
        "          # if the prediction matches the true label, update the correct count for this tag\n",
        "          correct_per_class[true] = correct_per_class.get(true, 0) + 1 if pred == true else correct_per_class.get(true, 0)\n",
        "\n",
        "\n",
        "        # trak the L2 norm of the gradient for each model parameter\n",
        "        for name, param in model.named_parameters():\n",
        "\n",
        "          # only consider parameters that have gradients\n",
        "          if param.grad is not None:\n",
        "\n",
        "              # compute the L2 norm\n",
        "              norm = param.grad.data.norm(2).item()\n",
        "\n",
        "              # if this parameter hasn't been seen yet, initialize a list to store its gradient norms\n",
        "              if name not in grad_norms:\n",
        "                  grad_norms[name] = []\n",
        "\n",
        "              # Append the current gradient norm to the list for this parameter\n",
        "              grad_norms[name].append(norm)\n",
        "\n",
        "    # Store average training loss and accuracy for this epoch\n",
        "    metrics['loss']['train'].append(total_loss / len(dataloader_train))\n",
        "    metrics['accuracy']['train'].append(total_correct / max(total_tokens, 1))\n",
        "\n",
        "    # Update tqdm bar with current epoch metrics\n",
        "    epoch_bar.set_postfix({\n",
        "    'Train Loss': f\"{metrics['loss']['train'][-1]:.4f}\",\n",
        "    'Train Acc': f\"{metrics['accuracy']['train'][-1]:.4f}\"})\n",
        "\n",
        "\n",
        "    # set mode to training mode\n",
        "    model.eval()\n",
        "\n",
        "    # initialize the metrics for the epoch on dev\n",
        "    dev_loss, dev_correct, dev_tokens = 0, 0, 0\n",
        "\n",
        "    # disable gradient tracking for evaluation\n",
        "    with th.no_grad():\n",
        "        for x_batch, casing_batch, y_batch in dataloader_dev:\n",
        "\n",
        "            # verifies dev in the same device\n",
        "            x_batch, casing_batch, y_batch = [x.to(DEVICE)\n",
        "                                               for x in (x_batch, casing_batch, y_batch)]\n",
        "\n",
        "            # forward pass to dev data and flatten output to match the CrossEntropy\n",
        "            logits = model((x_batch, casing_batch, y_batch))\n",
        "            logits = logits.view(-1, logits.size(-1))\n",
        "            targets = y_batch.view(-1)\n",
        "\n",
        "            # compute the dev metrics\n",
        "            dev_loss += loss_fn(logits, targets).item()\n",
        "            preds = logits.argmax(dim=1)\n",
        "            mask = targets != PAD_TOKEN\n",
        "            dev_correct += ((preds == targets) & mask).sum().item()\n",
        "            dev_tokens  += mask.sum().item()\n",
        "\n",
        "    # average dev metrics for this epoch\n",
        "    metrics['loss']['dev'].append(dev_loss / len(dataloader_dev))\n",
        "    metrics['accuracy']['dev'].append(dev_correct / max(dev_tokens, 1))\n",
        "\n",
        "    # Update tqdm bar with both train and dev metrics\n",
        "    epoch_bar.set_postfix({\n",
        "        'Train Loss': f\"{metrics['loss']['train'][-1]:.4f}\",\n",
        "        'Train Acc' : f\"{metrics['accuracy']['train'][-1]:.4f}\",\n",
        "        'Dev Loss'  : f\"{metrics['loss']['dev'][-1]:.4f}\",\n",
        "        'Dev Acc'   : f\"{metrics['accuracy']['dev'][-1]:.4f}\"\n",
        "    })\n",
        "\n",
        "\n",
        "\n",
        "  # TO DO ----------------------------------------------------------------------\n",
        "  return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2KC1nYSi18PV"
      },
      "outputs": [],
      "source": [
        "# metrics = train_loop(model, n_epochs=5, dataloader_train=dl_train, dataloader_dev=dl_dev)\n",
        "# metrics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_training_metrics(metrics):\n",
        "    epochs = range(1, len(metrics['loss']['train']) + 1)\n",
        "\n",
        "    # Plot Loss\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, metrics['loss']['train'], label='Train Loss')\n",
        "    plt.plot(epochs, metrics['loss']['dev'], label='Dev Loss')\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Loss over Epochs\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Plot Accuracy\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, metrics['accuracy']['train'], label='Train Accuracy')\n",
        "    plt.plot(epochs, metrics['accuracy']['dev'], label='Dev Accuracy')\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.title(\"Accuracy over Epochs\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "# metrics = train_loop(model, n_epochs, dataloader_train, dataloader_dev)\n",
        "# plot_training_metrics(metrics)"
      ],
      "metadata": {
        "id": "8pT3tgurI8qs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9C9su31_r7F"
      },
      "source": [
        "<br><br><br><br><br><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqSMEZDK9OPY"
      },
      "source": [
        "# Part 3 - Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baN1c_B7lTjb"
      },
      "source": [
        "\n",
        "## Step 1: Evaluation Function\n",
        "\n",
        "Write an evaluation loop for a trained model using the dev and test datasets. This function will print the `Recall`, `Precision`, and `F1` scores and plot a `Confusion Matrix`.\n",
        "\n",
        "Perform this evaluation twice:\n",
        "1. For all labels (7 labels in total).\n",
        "2. For all labels except \"O\" (6 labels in total)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6MhtYgh7hDO"
      },
      "source": [
        "## Metrics and Display\n",
        "\n",
        "### Metrics\n",
        "- **Recall**: True Positive Rate (TPR), also known as Recall.\n",
        "- **Precision**: The opposite of False Positive Rate (FPR), also known as Precision.\n",
        "- **F1 Score**: The harmonic mean of Precision and Recall.\n",
        "\n",
        "*Note*: For all these metrics, use **weighted** averaging:\n",
        "Calculate metrics for each label, and find their average weighted by support. Refer to the [sklearn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html#sklearn.metrics.precision_recall_fscore_support) for more details.\n",
        "\n",
        "### Display\n",
        "1. Print the `Recall`, `Precision`, and `F1` scores in a tabulated format.\n",
        "2. Display a `Confusion Matrix` plot:\n",
        "   - Rows represent the predicted labels.\n",
        "   - Columns represent the true labels.\n",
        "   - Include a title for the plot, axis names, and the names of the tags on the X-axis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iyQAjGaqmd8U"
      },
      "outputs": [],
      "source": [
        "def evaluate(model: NERNet, title: str, dataloader: DataLoader, vocab: Vocab):\n",
        "  \"\"\"\n",
        "  Evaluate a trained model on the given dataset.\n",
        "  :param model: model instance\n",
        "  :param title: title for the plot\n",
        "  :param dataloader: dataloader\n",
        "  :param vocab: Vocab instance\n",
        "  :return: Dictionary of evaluation results\n",
        "  \"\"\"\n",
        "  results = {}\n",
        "  # TO DO ----------------------------------------------------------------------\n",
        "\n",
        "  # make sure the model in evaluation mode to prevent gradient tracking\n",
        "  model.eval()\n",
        "\n",
        "  # init lists for all predictions and ground-truths\n",
        "  all_preds, all_labels = [], []\n",
        "\n",
        "  # verifies no gradient computations + right device for all batches\n",
        "  with th.no_grad():\n",
        "        for x_batch, casing_batch, y_batch in dataloader:\n",
        "            x_batch, casing_batch, y_batch = [x.to(DEVICE) for x in (x_batch, casing_batch, y_batch)]\n",
        "\n",
        "            # forward pass and choosing the most likely tag\n",
        "            logits = model((x_batch, casing_batch, y_batch))\n",
        "            preds = logits.argmax(dim=-1)\n",
        "\n",
        "            # flatten all predictions and skip the padding tokens\n",
        "            for pred_seq, true_seq in zip(preds, y_batch):\n",
        "                for p, t in zip(pred_seq, true_seq):\n",
        "                    if t.item() != PAD_TOKEN:\n",
        "                        all_preds.append(p.item())\n",
        "                        all_labels.append(t.item())\n",
        "\n",
        "            # calculate precision, recall, and F1 using sklearn.metrics on the flattend data\n",
        "            precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "                all_labels, all_preds, labels=list(vocab.tag2id.values()), average='weighted'\n",
        "            )\n",
        "\n",
        "            # store the metrics in the results dictionary\n",
        "            results['Precision'] = precision\n",
        "            results['Recall'] = recall\n",
        "            results['F1'] = f1\n",
        "\n",
        "            cm = confusion_matrix(all_labels, all_preds, labels=list(vocab.tag2id.values()))\n",
        "            results['ConfusionMatrix'] = cm\n",
        "\n",
        "        # print classification metric summary\n",
        "        print(f\"--- {title} ---\")\n",
        "        print(f\"Precision: {precision:.4f}\")\n",
        "        print(f\"Recall   : {recall:.4f}\")\n",
        "        print(f\"F1 Score : {f1:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # TO DO ----------------------------------------------------------------------\n",
        "  return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trn5-50FyGoF"
      },
      "source": [
        "## Step 2: Train & Evaluate on Dev Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQSXqWNOmqG4"
      },
      "source": [
        "Train and evaluate (on the dev set) a few models, all with `embedding_size=300` and `N_EPOCHS=5` (for fairness and computational reasons), and with the following hyper parameters (you may use that as captions for the models as well):\n",
        "\n",
        "- Model 1: (hidden_size: 500, n_layers: 1, directions: 1)\n",
        "- Model 2: (hidden_size: 500, n_layers: 2, directions: 1)\n",
        "- Model 3: (hidden_size: 500, n_layers: 3, directions: 1)\n",
        "- Model 4: (hidden_size: 500, n_layers: 1, directions: 2)\n",
        "- Model 5: (hidden_size: 500, n_layers: 2, directions: 2)\n",
        "- Model 6: (hidden_size: 500, n_layers: 3, directions: 2)\n",
        "- Model 7: (hidden_size: 800, n_layers: 1, directions: 2)\n",
        "- Model 8: (hidden_size: 800, n_layers: 2, directions: 2)\n",
        "- Model 9: (hidden_size: 800, n_layers: 3, directions: 2)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3RcWmNAsoTj"
      },
      "outputs": [],
      "source": [
        "N_EPOCHS = 5\n",
        "EMB_DIM = 300"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCaaeKV2CZF-"
      },
      "source": [
        "Here is an example (random numbers) of the display of the results):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Im49dRJeCjdv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2efae149-ef5a-4989-8e01-47a9cbd04870"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-----------+---------------+------------+--------------+----------+-------------+--------+---------------+------------------+-----------+\n",
            "|    | N_MODEL   |   HIDDEN_SIZE |   N_LAYERS |   DIRECTIONS |   RECALL |   PERCISION |     F1 |   RECALL_WO_O |   PERCISION_WO_O |   F1_WO_O |\n",
            "|----+-----------+---------------+------------+--------------+----------+-------------+--------+---------------+------------------+-----------|\n",
            "|  0 | model_1   |        0.9507 |     0.7320 |       0.5987 |   0.1560 |      0.1560 | 0.0581 |        0.8662 |           0.6011 |    0.7081 |\n",
            "|  1 | model_2   |        0.9699 |     0.8324 |       0.2123 |   0.1818 |      0.1834 | 0.3042 |        0.5248 |           0.4319 |    0.2912 |\n",
            "|  2 | model_3   |        0.1395 |     0.2921 |       0.3664 |   0.4561 |      0.7852 | 0.1997 |        0.5142 |           0.5924 |    0.0465 |\n",
            "|  3 | model_4   |        0.1705 |     0.0651 |       0.9489 |   0.9656 |      0.8084 | 0.3046 |        0.0977 |           0.6842 |    0.4402 |\n",
            "|  4 | model_5   |        0.4952 |     0.0344 |       0.9093 |   0.2588 |      0.6625 | 0.3117 |        0.5201 |           0.5467 |    0.1849 |\n",
            "|  5 | model_6   |        0.7751 |     0.9395 |       0.8948 |   0.5979 |      0.9219 | 0.0885 |        0.1960 |           0.0452 |    0.3253 |\n",
            "|  6 | model_7   |        0.2713 |     0.8287 |       0.3568 |   0.2809 |      0.5427 | 0.1409 |        0.8022 |           0.0746 |    0.9869 |\n",
            "|  7 | model_8   |        0.1987 |     0.0055 |       0.8155 |   0.7069 |      0.7290 | 0.7713 |        0.0740 |           0.3585 |    0.1159 |\n",
            "|  8 | model_9   |        0.6233 |     0.3309 |       0.0636 |   0.3110 |      0.3252 | 0.7296 |        0.6376 |           0.8872 |    0.4722 |\n",
            "+----+-----------+---------------+------------+--------------+----------+-------------+--------+---------------+------------------+-----------+\n"
          ]
        }
      ],
      "source": [
        "# Example:\n",
        "results_acc = np.random.rand(9, 10)\n",
        "columns = ['N_MODEL','HIDDEN_SIZE','N_LAYERS','DIRECTIONS','RECALL','PERCISION','F1','RECALL_WO_O','PERCISION_WO_O','F1_WO_O']\n",
        "df = pd.DataFrame(results_acc, columns=columns)\n",
        "df.N_MODEL = [f'model_{n}' for n in range(1,10)]\n",
        "print(tabulate(df, headers='keys', tablefmt='psql',floatfmt=\".4f\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1p7_f3JCeP2"
      },
      "outputs": [],
      "source": [
        "# Define models with their hyperparameters\n",
        "models = {\n",
        "  'Model1': {'embedding_size': EMB_DIM, 'hidden_size': 500, 'n_layers': 1, 'directions': 1},\n",
        "  'Model2': {'embedding_size': EMB_DIM, 'hidden_size': 500, 'n_layers': 2, 'directions': 1},\n",
        "  'Model3': {'embedding_size': EMB_DIM, 'hidden_size': 500, 'n_layers': 3, 'directions': 1},\n",
        "  'Model4': {'embedding_size': EMB_DIM, 'hidden_size': 500, 'n_layers': 1, 'directions': 2},\n",
        "  'Model5': {'embedding_size': EMB_DIM, 'hidden_size': 500, 'n_layers': 2, 'directions': 2},\n",
        "  'Model6': {'embedding_size': EMB_DIM, 'hidden_size': 500, 'n_layers': 3, 'directions': 2},\n",
        "  'Model7': {'embedding_size': EMB_DIM, 'hidden_size': 800, 'n_layers': 1, 'directions': 2},\n",
        "  'Model8': {'embedding_size': EMB_DIM, 'hidden_size': 800, 'n_layers': 2, 'directions': 2},\n",
        "  'Model9': {'embedding_size': EMB_DIM, 'hidden_size': 800, 'n_layers': 3, 'directions': 2},\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TO DO ----------------------------------------------------------------------\n",
        "\n",
        "# storage for the final metrics\n",
        "results_dev = []\n",
        "\n",
        "for name, config in models.items():\n",
        "  print(f\"\\n================ {name} =================\")\n",
        "  # build a model based on the specific configurations\n",
        "  model = NERNet(\n",
        "        input_size   = vocab.n_words,\n",
        "        output_size  = vocab.n_tags,\n",
        "        **config        # expands into ethe reauired parameters\n",
        "    ).to(DEVICE)\n",
        "\n",
        "  # train the model\n",
        "  train_loop(model, n_epochs=N_EPOCHS,\n",
        "               dataloader_train = dl_train,\n",
        "               dataloader_dev   = dl_dev)\n",
        "\n",
        "  # evaluate on dev\n",
        "  dev_res = evaluate(model, title=f\"{name} ‚Äì Dev\", dataloader=dl_dev, vocab=vocab)\n",
        "  results_dev.append({\n",
        "      \"Model\"     : name,\n",
        "      \"Precision\" : dev_res[\"Precision\"],\n",
        "      \"Recall\"    : dev_res[\"Recall\"],\n",
        "      \"F1\"        : dev_res[\"F1\"],\n",
        "  })\n",
        "\n",
        "# TO DO ----------------------------------------------------------------------\n",
        "\n",
        "# Print results in tabulated format\n",
        "print(tabulate(results_dev, headers='keys', tablefmt='psql', floatfmt=\".4f\"))"
      ],
      "metadata": {
        "id": "gCV_zailBM8P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a769b547-f3e1-4392-eacc-0785279f2845"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================ Model1 =================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:13<00:00,  2.76s/it, Train Loss=0.6628, Train Acc=0.8238, Dev Loss=0.6923, Dev Acc=0.8157]\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Model1 ‚Äì Dev ---\n",
            "Precision: 0.7679\n",
            "Recall   : 0.8157\n",
            "F1 Score : 0.7555\n",
            "\n",
            "================ Model2 =================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:11<00:00,  2.29s/it, Train Loss=0.5332, Train Acc=0.8459, Dev Loss=0.5649, Dev Acc=0.8407]\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Model2 ‚Äì Dev ---\n",
            "Precision: 0.7735\n",
            "Recall   : 0.8407\n",
            "F1 Score : 0.7925\n",
            "\n",
            "================ Model3 =================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:14<00:00,  2.96s/it, Train Loss=0.6043, Train Acc=0.8273, Dev Loss=0.5691, Dev Acc=0.8417]\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Model3 ‚Äì Dev ---\n",
            "Precision: 0.7686\n",
            "Recall   : 0.8417\n",
            "F1 Score : 0.7981\n",
            "\n",
            "================ Model4 =================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:10<00:00,  2.15s/it, Train Loss=0.4512, Train Acc=0.8683, Dev Loss=0.4932, Dev Acc=0.8606]\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Model4 ‚Äì Dev ---\n",
            "Precision: 0.8323\n",
            "Recall   : 0.8606\n",
            "F1 Score : 0.8257\n",
            "\n",
            "================ Model5 =================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:20<00:00,  4.16s/it, Train Loss=0.2090, Train Acc=0.9416, Dev Loss=0.2673, Dev Acc=0.9210]\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Model5 ‚Äì Dev ---\n",
            "Precision: 0.9154\n",
            "Recall   : 0.9210\n",
            "F1 Score : 0.9136\n",
            "\n",
            "================ Model6 =================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:30<00:00,  6.07s/it, Train Loss=0.1587, Train Acc=0.9514, Dev Loss=0.2356, Dev Acc=0.9337]\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Model6 ‚Äì Dev ---\n",
            "Precision: 0.9290\n",
            "Recall   : 0.9337\n",
            "F1 Score : 0.9284\n",
            "\n",
            "================ Model7 =================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:19<00:00,  3.84s/it, Train Loss=0.3695, Train Acc=0.8927, Dev Loss=0.4208, Dev Acc=0.8794]\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Model7 ‚Äì Dev ---\n",
            "Precision: 0.8687\n",
            "Recall   : 0.8794\n",
            "F1 Score : 0.8576\n",
            "\n",
            "================ Model8 =================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:43<00:00,  8.74s/it, Train Loss=0.1375, Train Acc=0.9595, Dev Loss=0.2057, Dev Acc=0.9350]\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Model8 ‚Äì Dev ---\n",
            "Precision: 0.9310\n",
            "Recall   : 0.9350\n",
            "F1 Score : 0.9320\n",
            "\n",
            "================ Model9 =================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [01:10<00:00, 14.08s/it, Train Loss=0.1040, Train Acc=0.9662, Dev Loss=0.2274, Dev Acc=0.9294]\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Model9 ‚Äì Dev ---\n",
            "Precision: 0.9343\n",
            "Recall   : 0.9294\n",
            "F1 Score : 0.9313\n",
            "+---------+-------------+----------+--------+\n",
            "| Model   |   Precision |   Recall |     F1 |\n",
            "|---------+-------------+----------+--------|\n",
            "| Model1  |      0.7679 |   0.8157 | 0.7555 |\n",
            "| Model2  |      0.7735 |   0.8407 | 0.7925 |\n",
            "| Model3  |      0.7686 |   0.8417 | 0.7981 |\n",
            "| Model4  |      0.8323 |   0.8606 | 0.8257 |\n",
            "| Model5  |      0.9154 |   0.9210 | 0.9136 |\n",
            "| Model6  |      0.9290 |   0.9337 | 0.9284 |\n",
            "| Model7  |      0.8687 |   0.8794 | 0.8576 |\n",
            "| Model8  |      0.9310 |   0.9350 | 0.9320 |\n",
            "| Model9  |      0.9343 |   0.9294 | 0.9313 |\n",
            "+---------+-------------+----------+--------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4o5joZgA1YE"
      },
      "source": [
        "## Step 3: Evaluate on Test Set\n",
        "Evaluate your models on the test set and save the results as a CSV. Add this file to your repo for submission."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTNmBU6hycZl",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ec5bc1f-c1ff-44bb-da22-e93a97d844b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+---------------+------------+--------------+----------+-------------+------+---------------+------------------+-----------+\n",
            "| N_MODEL   | HIDDEN_SIZE   | N_LAYERS   | DIRECTIONS   | RECALL   | PERCISION   | F1   | RECALL_WO_O   | PERCISION_WO_O   | F1_WO_O   |\n",
            "|-----------+---------------+------------+--------------+----------+-------------+------+---------------+------------------+-----------|\n",
            "+-----------+---------------+------------+--------------+----------+-------------+------+---------------+------------------+-----------+\n"
          ]
        }
      ],
      "source": [
        "results = pd.DataFrame(columns=columns)\n",
        "file_name = \"NER_results.csv\"\n",
        "# TO DO ----------------------------------------------------------------------\n",
        "\n",
        "# storage for the final metrics\n",
        "metrics_test = []\n",
        "\n",
        "# iterates over all\n",
        "for name, model in models.items():\n",
        "    model.to(DEVICE)\n",
        "    metrics = evaluate(model, name, dl_test, vocab)   # ‚Üê the only change: dl_test\n",
        "    metrics_test.append({\n",
        "        \"Model\":     name,\n",
        "        \"Precision\": metrics[\"precision\"],\n",
        "        \"Recall\":    metrics[\"recall\"],\n",
        "        \"F1\":        metrics[\"f1\"]\n",
        "    })\n",
        "\n",
        "\n",
        "# make a dataFrame and save the results as csv\n",
        "results_test = pd.DataFrame(metrics_test, columns=[\"Model\",\"Precision\",\"Recall\",\"F1\"])\n",
        "\n",
        "# TO DO ----------------------------------------------------------------------\n",
        "print(tabulate(results, headers='keys', tablefmt='psql',floatfmt=\".4f\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Y4U4eHmShw8"
      },
      "source": [
        "## Step 4 - Best Model\n",
        "Decide which model performs the best, write its configuration, train it for 5 more epochs and evaluate it on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3TnKYqmSvds"
      },
      "outputs": [],
      "source": [
        "best_model_cfg = {'embedding_size':EMB_DIM, 'hidden_size': -1, 'n_layers': -1, 'directions': -1}\n",
        "# TO DO ----------------------------------------------------------------------\n",
        "\n",
        "# TO DO ----------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsCfk8caB3iU"
      },
      "source": [
        "<br><br><br><br><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nF_jrCkpCVUD"
      },
      "source": [
        "# Part 4 - Pretrained Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UM74r0_8nk5s"
      },
      "source": [
        "\n",
        "\n",
        "To prepare for this task, please read [this discussion](https://discuss.pytorch.org/t/can-we-use-pre-trained-word-embeddings-for-weight-initialization-in-nn-embedding/1222).\n",
        "\n",
        "**TIP**: Ensure that the vectors are aligned with the IDs in your vocabulary. In other words, make sure that the word with ID 0 corresponds to the first vector in the GloVe matrix used to initialize `nn.Embedding`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6rrSb-bFoTa"
      },
      "source": [
        "## Step 1: Get Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dscQwqxvBP5-"
      },
      "source": [
        "\n",
        "\n",
        "Download the GloVe embeddings from [this link](https://nlp.stanford.edu/projects/glove/). Use the 300-dimensional vectors from `glove.6B.zip`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbY3bBiH_9E8"
      },
      "outputs": [],
      "source": [
        "# TO DO ----------------------------------------------------------------------\n",
        "\n",
        "# TO DO ----------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBLEK9R3Fxye"
      },
      "source": [
        "## Step 2: Inject Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xH8ybhqY_8US"
      },
      "source": [
        "Then intialize the `nn.Embedding` module in your `NERNet` with these embeddings, so that you can start your training with pre-trained vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXSEIjfe9DSy"
      },
      "outputs": [],
      "source": [
        "def get_emb_matrix(filepath: str, vocab: Vocab) -> np.ndarray:\n",
        "  emb_matrix = np.zeros((len(vocab.word2id), 300))\n",
        "  # TO DO ----------------------------------------------------------------------\n",
        "\n",
        "  # TO DO ----------------------------------------------------------------------\n",
        "  return emb_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I56mqREnVRey"
      },
      "outputs": [],
      "source": [
        "def initialize_from_pretrained_emb(model: NERNet, emb_matrix: np.ndarray):\n",
        "  \"\"\"\n",
        "  Inject the pretrained embeddings into the model.\n",
        "  :param model: model instance\n",
        "  :param emb_matrix: pretrained embeddings\n",
        "  \"\"\"\n",
        "  # TO DO ----------------------------------------------------------------------\n",
        "\n",
        "  # TO DO ----------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lf0byP9y_9jG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "654ca3f5-1fa6-453f-b12f-af8ae4a0900f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'VOCAB_SIZE' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-0c40d066eea4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0memb_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'glove.6B.300d.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0memb_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_emb_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mner_glove\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNERNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mVOCAB_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEMB_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_TAGS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0minitialize_from_pretrained_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mner_glove\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'VOCAB_SIZE' is not defined"
          ]
        }
      ],
      "source": [
        "# Read embeddings and inject them to a model\n",
        "emb_file = 'glove.6B.300d.txt'\n",
        "emb_matrix = get_emb_matrix(emb_file, vocab)\n",
        "ner_glove = NERNet(input_size=VOCAB_SIZE, embedding_size=EMB_DIM, hidden_size=500, output_size=NUM_TAGS, n_layers=1, directions=1)\n",
        "initialize_from_pretrained_emb(ner_glove, emb_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JElogtlHF4DN"
      },
      "source": [
        "## Step 3: Evaluate on Test Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Obm_WN4_ALmM"
      },
      "source": [
        "Same as the evaluation process before, please display:\n",
        "\n",
        "1. Print a `RECALL-PERCISION-F1` scores in a tabulate format.\n",
        "2. Display a `confusion matrix` plot: where the predicted labels are the rows, and the true labels are the columns.\n",
        "\n",
        "Make sure to use the title for the plot, axis names, and the names of the tags on the X-axis.\n",
        "\n",
        "Make sure to download and upload this CSV as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7Xw3h6mAhRw"
      },
      "outputs": [],
      "source": [
        "results = pd.DataFrame(columns=columns)\n",
        "file_name = \"NER_results_glove.csv\"\n",
        "# TO DO ----------------------------------------------------------------------\n",
        "\n",
        "# TO DO ----------------------------------------------------------------------\n",
        "print(tabulate(results, headers='keys', tablefmt='psql',floatfmt=\".4f\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5XrhAdNXShg"
      },
      "source": [
        "## Step 4 - Best Model\n",
        "Decide which model performs the best, write its configuration, train it for 5 more epochs and evaluate it on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHI_ECNwXR-7"
      },
      "outputs": [],
      "source": [
        "best_model_glove_cfg = {'embedding_size':EMB_DIM, 'hidden_size': -1, 'n_layers': -1, 'directions': -1}\n",
        "# TO DO ----------------------------------------------------------------------\n",
        "\n",
        "# TO DO ----------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 5 - Error Analysis"
      ],
      "metadata": {
        "id": "SwHbsC7MgMEd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this part, you'll analyze the errors made by your best model to understand its strengths and weaknesses."
      ],
      "metadata": {
        "id": "QLtktllTgVpb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Extract Predictions\n",
        "\n",
        "First, let's extract predictions from your best model on the test set:"
      ],
      "metadata": {
        "id": "zpEwdF8oghJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_predictions(model, dataloader, vocab, PAD_TOKEN, DEVICE):\n",
        "    \"\"\"\n",
        "    Get predictions from the model on a dataloader.\n",
        "\n",
        "    Returns:\n",
        "        - true_tags_list: List of lists of true tag strings\n",
        "        - pred_tags_list: List of lists of predicted tag strings\n",
        "        - words_list: List of lists of words\n",
        "    \"\"\"\n",
        "    import torch\n",
        "\n",
        "    model.eval()\n",
        "    true_tags_list = []\n",
        "    pred_tags_list = []\n",
        "    words_list = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Handle different dataloader output formats\n",
        "        for batch in dataloader:\n",
        "            # Unpack based on actual dataloader output\n",
        "            if len(batch) == 3:  # (input_ids, casing_features, labels)\n",
        "                input_ids, casing_features, labels = batch\n",
        "                # Move tensors to device\n",
        "                input_ids = input_ids.to(DEVICE)\n",
        "                casing_features = casing_features.to(DEVICE)\n",
        "                labels = labels.to(DEVICE)\n",
        "\n",
        "                # Get model predictions\n",
        "                outputs = model(input_ids, casing_features)\n",
        "            else:  # (input_ids, labels)\n",
        "                input_ids, labels = batch\n",
        "                # Move tensors to device\n",
        "                input_ids = input_ids.to(DEVICE)\n",
        "                labels = labels.to(DEVICE)\n",
        "\n",
        "                # Get model predictions\n",
        "                outputs = model(input_ids)\n",
        "\n",
        "            _, predicted = torch.max(outputs, 2)\n",
        "\n",
        "            # Process each sequence in the batch\n",
        "            for i in range(input_ids.size(0)):\n",
        "                # Get sequence length (ignoring padding)\n",
        "                seq_len = (input_ids[i] != PAD_TOKEN).sum().item()\n",
        "\n",
        "                # Convert ids to tag strings and words\n",
        "                true_tags = [vocab.id2tag[tag.item()] for tag in labels[i][:seq_len]]\n",
        "                pred_tags = [vocab.id2tag[tag.item()] for tag in predicted[i][:seq_len]]\n",
        "                words = [vocab.id2word[word.item()] for word in input_ids[i][:seq_len]]\n",
        "\n",
        "                true_tags_list.append(true_tags)\n",
        "                pred_tags_list.append(pred_tags)\n",
        "                words_list.append(words)\n",
        "\n",
        "    return true_tags_list, pred_tags_list, words_list"
      ],
      "metadata": {
        "id": "gzWerE5rzjsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Implement Simple Error Analysis\n",
        "\n",
        "Now, implement a function to analyze the errors in predictions:"
      ],
      "metadata": {
        "id": "rBT9owrYgr_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def simple_analyze_errors(true_tags, pred_tags, words):\n",
        "    \"\"\"\n",
        "    Analyze errors in NER predictions.\n",
        "\n",
        "    Args:\n",
        "        true_tags: List of true tag sequences\n",
        "        pred_tags: List of predicted tag sequences\n",
        "        words: List of word sequences\n",
        "\n",
        "    Returns:\n",
        "        dict: Error statistics and examples\n",
        "    \"\"\"\n",
        "    # TODO: Implement error analysis\n",
        "    # 1. Initialize error categories\n",
        "    # 2. Process each sequence to identify errors\n",
        "    # 3. Categorize errors and collect examples\n",
        "    # 4. Return statistics and examples\n",
        "\n",
        "    # Placeholder\n",
        "    return {\n",
        "        'total_entities': 0,\n",
        "        'correct_entities': 0,\n",
        "        'accuracy': 0.0,\n",
        "        'error_counts': {},\n",
        "        'error_examples': {}\n",
        "    }"
      ],
      "metadata": {
        "id": "WNalBXJbgt6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Helper Functions\n",
        "\n",
        "Implement these helper functions to extract entities and check for overlapping spans:"
      ],
      "metadata": {
        "id": "Uv_tWhNYg7YP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_entities_simple(tags):\n",
        "    \"\"\"\n",
        "    Extract entities from a sequence of tags.\n",
        "    Returns list of (start_idx, end_idx, entity_type) tuples.\n",
        "    \"\"\"\n",
        "    # TODO: Implement entity extraction\n",
        "    return []\n",
        "\n",
        "def has_overlap(start1, end1, start2, end2):\n",
        "    \"\"\"Check if two spans overlap\"\"\"\n",
        "    # TODO: Implement overlap checking\n",
        "    return False"
      ],
      "metadata": {
        "id": "6X75UgM2meDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Visualization and Analysis\n",
        "\n",
        "Create a function to display the error analysis results:"
      ],
      "metadata": {
        "id": "qG-HSI_S3se4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_error_analysis(analysis):\n",
        "    \"\"\"Print a summary of the error analysis results\"\"\"\n",
        "    # TODO: Implement printing function to show:\n",
        "    # 1. Basic statistics (total entities, correct entities, accuracy)\n",
        "    # 2. Error counts by category\n",
        "    # 3. Examples of each error type\n",
        "    # 4. Suggestions for improvement based on findings\n",
        "    pass"
      ],
      "metadata": {
        "id": "E5MI6Wvn3tIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Improvement Suggestions\n",
        "\n",
        "Based on your error analysis, suggest at least three specific improvements to your model. Consider:\n",
        "\n",
        "1. What types of errors are most common?\n",
        "2. Are there patterns in the errors (e.g., specific entity types, contexts)?\n",
        "3. What techniques might address these specific error types?\n",
        "\n",
        "Write your suggestions in 3-5 sentences for each improvement."
      ],
      "metadata": {
        "id": "h5_bV_Eb34M_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Sample data for testing\n",
        "    true_tags = [\n",
        "        ['O', 'B-PER', 'I-PER', 'O', 'B-LOC', 'I-LOC', 'O'],\n",
        "        ['B-ORG', 'I-ORG', 'O', 'B-PER', 'O']\n",
        "    ]\n",
        "\n",
        "    pred_tags = [\n",
        "        ['O', 'B-PER', 'O', 'O', 'B-ORG', 'I-ORG', 'O'],\n",
        "        ['B-ORG', 'I-ORG', 'I-ORG', 'B-PER', 'O']\n",
        "    ]\n",
        "\n",
        "    words = [\n",
        "        ['The', 'John', 'Smith', 'visited', 'New', 'York', 'yesterday'],\n",
        "        ['Google', 'Inc', 'hired', 'Alice', 'recently']\n",
        "    ]\n",
        "\n",
        "    # Run the error analysis\n",
        "    analysis = simple_analyze_errors(true_tags_list, pred_tags_list, words_list)\n",
        "    print_error_analysis(analysis)\n",
        "\n",
        "    # TODO: Write your improvement suggestions here"
      ],
      "metadata": {
        "id": "9OPBb6vV44JR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember to focus on substantive analysis rather than code complexity. Your goal is to identify meaningful patterns and provide practical suggestions for improvement."
      ],
      "metadata": {
        "id": "A_YAo8Mn4B4X"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cUiLd_gCRO_"
      },
      "source": [
        "# Testing\n",
        "Copy the content of the **tests.py** file from the repo and paste below. This will create the results.json file and download it to your machine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMVrbY0SCRja"
      },
      "outputs": [],
      "source": [
        "####################\n",
        "# PLACE TESTS HERE #\n",
        "train_ds = read_data(\"data/train.txt\")\n",
        "dev_ds = read_data(\"data/dev.txt\")\n",
        "test_ds = read_data(\"data/test.txt\")\n",
        "def test_read_data():\n",
        "    result = {\n",
        "        'lengths': (len(train_ds), len(dev_ds), len(test_ds)),\n",
        "    }\n",
        "    return result\n",
        "\n",
        "vocab = Vocab(train_ds)\n",
        "def test_vocab():\n",
        "    sent = vocab.index_words([\"I\", \"am\", \"Spongebob\"])\n",
        "    return {\n",
        "        'length': vocab.n_words,\n",
        "        'tag2id_length': len(vocab.tag2id),\n",
        "        \"Spongebob\": sent[2]\n",
        "    }\n",
        "\n",
        "train_sequences = prepare_data(train_ds, vocab)\n",
        "dev_sequences = prepare_data(dev_ds, vocab)\n",
        "test_sequences = prepare_data(test_ds, vocab)\n",
        "\n",
        "def test_count_oov():\n",
        "    return {\n",
        "        'dev_oov': count_oov(dev_sequences),\n",
        "        'test_oov': count_oov(test_sequences)\n",
        "    }\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "dl_train = prepare_data_loader(train_sequences, batch_size=BATCH_SIZE)\n",
        "dl_dev = prepare_data_loader(dev_sequences, batch_size=BATCH_SIZE, train=False)\n",
        "dl_test = prepare_data_loader(test_sequences, batch_size=BATCH_SIZE, train=False)\n",
        "\n",
        "def test_prepare_data_loader():\n",
        "    return {\n",
        "        'lengths': (len(dl_train), len(dl_dev), len(dl_test))\n",
        "    }\n",
        "\n",
        "\n",
        "def test_NERNet():\n",
        "    # Extract best model configuration\n",
        "    hidden_size = best_model_cfg['hidden_size']\n",
        "    n_layers = best_model_cfg['n_layers']\n",
        "    directions = best_model_cfg['directions']\n",
        "\n",
        "\n",
        "    # Create model\n",
        "    best_model = NERNet(vocab.n_words, embedding_size=300, hidden_size=hidden_size, output_size=vocab.n_tags, n_layers=n_layers, directions=directions)\n",
        "    best_model.to(DEVICE)\n",
        "\n",
        "    # Train model and evaluate\n",
        "    _ = train_loop(best_model, n_epochs=10, dataloader_train=dl_train, dataloader_dev=dl_dev)\n",
        "    results = evaluate(best_model, title=\"\", dataloader=dl_test, vocab=vocab)\n",
        "\n",
        "    return {\n",
        "        'f1': results['F1'],\n",
        "        'f1_wo_o': results['F1_WO_O'],\n",
        "    }\n",
        "\n",
        "def test_glove():\n",
        "    # Get embeddings\n",
        "    emb_file = 'glove.6B.300d.txt'\n",
        "    emb_matrix = get_emb_matrix(emb_file, vocab)\n",
        "\n",
        "    # Extract best model configuration\n",
        "    hidden_size = best_model_glove_cfg['hidden_size']\n",
        "    n_layers = best_model_glove_cfg['n_layers']\n",
        "    directions = best_model_glove_cfg['directions']\n",
        "\n",
        "    # Create model\n",
        "    best_model = NERNet(vocab.n_words, embedding_size=300, hidden_size=hidden_size, output_size=vocab.n_tags, n_layers=n_layers, directions=directions)\n",
        "    best_model.to(DEVICE)\n",
        "    initialize_from_pretrained_emb(ner_glove, emb_matrix)\n",
        "\n",
        "    # Train model and evaluate\n",
        "    _ = train_loop(best_model, n_epochs=10, dataloader_train=dl_train, dataloader_dev=dl_dev)\n",
        "    results = evaluate(best_model, title=\"\", dataloader=dl_test, vocab=vocab)\n",
        "\n",
        "    return {\n",
        "        'f1': results['F1'],\n",
        "        'f1_wo_o': results['F1_WO_O'],\n",
        "    }\n",
        "\n",
        "TESTS = [\n",
        "    test_read_data,\n",
        "    test_vocab,\n",
        "    test_count_oov,\n",
        "    test_prepare_data_loader,\n",
        "    test_NERNet,\n",
        "    test_glove\n",
        "]\n",
        "\n",
        "# Run tests and save results\n",
        "res = {}\n",
        "for test in TESTS:\n",
        "    try:\n",
        "        cur_res = test()\n",
        "        res.update({test.__name__: cur_res})\n",
        "    except Exception as e:\n",
        "        res.update({test.__name__: repr(e)})\n",
        "\n",
        "with open('results.json', 'w') as f:\n",
        "    json.dump(res, f, indent=2)\n",
        "\n",
        "# Download the results.json file\n",
        "files.download('results.json')\n",
        "\n",
        "####################\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}